{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from parameters import parse_args\n",
    "from graph_utils import get_graph_data\n",
    "from data_generation_utils import get_kfold_lp_data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7469b9d59243a2a1184581209d9abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Masking graphs:   0%|          | 0/6219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9f162594c3473abc1db79d321d3fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding node strings to graphs:   0%|          | 0/6219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9c9a588ccc4bbc9e8f7147fd5be049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Masking graphs:   0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd682537a36d4f50972569fa380b07a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding node strings to graphs:   0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0ab929b0814aae90c0fafaa742ac9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting node triples:   0%|          | 0/6219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937f343c012c4e6e8686ed098a5b52f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting node triples:   0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Train triples [('StateMachine', 'states SMState', ''), ('SMState', 'transitions SMTransition', '')]\n",
      "Sample Test triples [('Workbench', 'things Thing, thoughts Thoughts, systemView System, functionProperties FunctionProperty', ''), ('RelatedTo', 'fromThing Thing', '')]\n",
      "Total entities: 51290\n",
      "Total super types: 7084\n",
      "Sample Train triples [('StateMachine', 'states SMState', ''), ('SMState', 'transitions SMTransition', '')]\n",
      "Sample Test triples [('Workbench', 'things Thing, thoughts Thoughts, systemView System, functionProperties FunctionProperty', ''), ('RelatedTo', 'fromThing Thing', '')]\n",
      "Total train triples: 105784\n",
      "Total test triples: 6839\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "data_dir = args.data_dir\n",
    "args.graphs_file = os.path.join(data_dir, args.graphs_file)\n",
    "\n",
    "\n",
    "graph_data = get_graph_data(args.graphs_file)\n",
    "label_map, super_type_map = graph_data['entities_encoder'], graph_data['super_types_encoder']\n",
    "inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "inverse_super_type_map = {v: k for k, v in super_type_map.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graphs:  5597 Test graphs:  622 Unseen graphs:  328\n"
     ]
    }
   ],
   "source": [
    "label_map, super_type_map = graph_data['entities_encoder'], graph_data['super_types_encoder']\n",
    "for i, data in enumerate(get_kfold_lp_data(graph_data)):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, unseen = data['train'], data['test'], data['unseen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import UMLGPT\n",
    "\n",
    "pth = 'models/super_PT_gpt2_s_pre_tok=bert-base-cased/super_PT_gpt2_s_pre_tok=bert-base-cased_best_model.pt'\n",
    "model = UMLGPT.from_pretrained(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 22:16:30.915287: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pretrained LM tokenizer...\n",
      "Vocab size:  29008\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from data_generation_utils import SPECIAL_TOKENS\n",
    "from trainers import get_tokenizer\n",
    "\n",
    "args.trainer = 'PT'\n",
    "args.special_tokens = SPECIAL_TOKENS\n",
    "\n",
    "tokenizer = get_tokenizer('bert-base-cased', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation_utils import promptize_triple\n",
    "\n",
    "promptize_node = lambda g, n: promptize_triple((n, g.nodes[n]['references'] if 'references' in g.nodes[n] else '', g.nodes[n]['super_types'] if 'super_types' in g.nodes[n] else ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('SMState', 'StateMachine'),\n",
       "  '<s> <superType>  </superType> <entity> SMState </entity> <relations> transitions SMTransition </relations> </s>',\n",
       "  '<s> <superType>  </superType> <entity> StateMachine </entity> <relations> states SMState </relations> </s>'),\n",
       " (('SMInstance', 'SMState'),\n",
       "  '<s> <superType>  </superType> <entity> SMInstance </entity> <relations> stateMachine StateMachine target EObject transitionInstances SMTransitionInstance </relations> </s>',\n",
       "  '<s> <superType>  </superType> <entity> SMState </entity> <relations> transitions SMTransition </relations> </s>')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(e, promptize_node(train_graph, e[0]), promptize_node(train_graph, e[1])) for e in train_graph.edges() if train_graph.edges[e]['masked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('SMState', 'StateMachine'), [101, 28998, 29002, 29003, 29004, 25345, 10237, 29005, 29006, 26829, 19293, 1942, 4047, 5053, 2116, 29007, 28999, 102], [101, 28998, 29002, 29003, 29004, 1426, 2107, 19226, 1673, 29005, 29006, 2231, 25345, 10237, 29007, 28999, 102]), (('SMInstance', 'SMState'), [101, 28998, 29002, 29003, 29004, 19293, 2240, 22399, 3923, 29005, 29006, 1352, 2107, 19226, 1673, 1426, 2107, 19226, 1673, 4010, 142, 2346, 24380, 6468, 2240, 22399, 3923, 1116, 19293, 1942, 4047, 5053, 2116, 2240, 22399, 3923, 29007, 28999, 102], [101, 28998, 29002, 29003, 29004, 25345, 10237, 29005, 29006, 26829, 19293, 1942, 4047, 5053, 2116, 29007, 28999, 102])]\n"
     ]
    }
   ],
   "source": [
    "print([(e, tokenizer.encode(promptize_node(train_graph, e[0])), tokenizer.encode(promptize_node(train_graph, e[1]))) for e in train_graph.edges() if train_graph.edges[e]['masked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "import dgl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_generation_utils import get_encoding_size\n",
    "from models import get_embedding\n",
    "\n",
    "\n",
    "def get_pos_neg_graphs(nxg, tr=0.2):\n",
    "    g = dgl.from_networkx(nxg, edge_attrs=['masked'])\n",
    "    u, v = g.edges()\n",
    "    test_mask = torch.where(g.edata['masked'])[0]\n",
    "    train_mask = torch.where(~g.edata['masked'])[0]\n",
    "    test_size = int(g.number_of_edges() * tr)\n",
    "    test_pos_u, test_pos_v = u[test_mask], v[test_mask]\n",
    "    train_pos_u, train_pos_v = u[train_mask], v[train_mask]\n",
    "\n",
    "    # Find all negative edges and split them for training and testing\n",
    "    adj = g.adjacency_matrix()\n",
    "    adj_neg = 1 - adj.to_dense() - np.eye(g.number_of_nodes())\n",
    "    neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "    neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
    "    test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "    train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
    "\n",
    "    train_g = dgl.remove_edges(g, test_mask)\n",
    "\n",
    "    train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "    train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "    test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "    test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n",
    "    \n",
    "    graphs = {\n",
    "        'train_pos_g': train_pos_g,\n",
    "        'train_neg_g': train_neg_g,\n",
    "        'test_pos_g': test_pos_g,\n",
    "        'test_neg_g': test_neg_g,\n",
    "        'train_g': train_g\n",
    "    }\n",
    "    return graphs\n",
    "\n",
    "\n",
    "class LinkPredictionDataset(DGLDataset):\n",
    "    def __init__(self, graphs, tokenizer, model, test_size=0.2, raw_dir='datasets/LP', save_dir='datasets/LP'):\n",
    "        self.raw_graphs = graphs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        super().__init__(name='link_prediction', raw_dir=raw_dir, save_dir=save_dir)\n",
    "        \"\"\"\n",
    "        Load dataset of graphs if exists, otherwise create it.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def process(self):\n",
    "        self.graphs = self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        prepared_graphs = [self._prepare_graph(g) for g in tqdm(self.raw_graphs, desc='Preparing graphs')]\n",
    "        return prepared_graphs\n",
    "    \n",
    "    def _prepare_graph(self, g):\n",
    "\n",
    "        node_strs = [promptize_node(g, n) for n in g.nodes()]\n",
    "        max_token_length = get_encoding_size(node_strs, tokenizer)\n",
    "        node_encodings = self.tokenizer(node_strs, padding=True, truncation=True, max_length=max_token_length, return_tensors='pt')\n",
    "        node_embeddings = get_embedding(model, node_encodings)\n",
    "        pos_neg_graphs = get_pos_neg_graphs(g, self.test_size)        \n",
    "        \n",
    "        dgl_graph = pos_neg_graphs['train_g']\n",
    "        dgl_graph.ndata['h'] = node_embeddings\n",
    "\n",
    "        return pos_neg_graphs\n",
    "\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save list of DGLGraphs using DGL save_graphs.\"\"\"\n",
    "        print(\"Saving graphs to cache...\")\n",
    "        keys = ['train_pos_g', 'train_neg_g', 'test_pos_g', 'test_neg_g', 'train_g']\n",
    "        graphs = {k: [g[k] for g in self.graphs] for k in keys}\n",
    "        for k, v in graphs.items():\n",
    "            dgl.save_graphs(os.path.join(self.save_dir, f'{self.name}_{k}.dgl'), v)\n",
    "    \n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Load list of DGLGraphs using DGL load_graphs.\"\"\"\n",
    "        print(\"Loading graphs from cache...\")\n",
    "        \n",
    "        keys = ['train_pos_g', 'train_neg_g', 'test_pos_g', 'test_neg_g', 'train_g']\n",
    "        k_graphs = {k: [] for k in keys}\n",
    "        for k in keys:\n",
    "            k_graphs[k] = dgl.load_graphs(os.path.join(self.save_dir, f'{self.name}_{k}.dgl'))[0]\n",
    "        \n",
    "        self.graphs = list()\n",
    "        for i in range(len(k_graphs['train_g'])):\n",
    "            self.graphs.append({k: v[i] for k, v in k_graphs.items()})\n",
    "        \n",
    "        print(f'Loaded {len(self.graphs)} graphs.')\n",
    "\n",
    "        \n",
    "    def has_cache(self):\n",
    "        return os.path.exists(os.path.join(self.save_dir, f'{self.name}_train_g.dgl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bec3d2e74d49718e7036cd17607dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing graphs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding size:  38\n",
      "Encoding size:  19\n",
      "Encoding size:  86\n",
      "Encoding size:  44\n",
      "Saving graphs to cache...\n"
     ]
    }
   ],
   "source": [
    "dataset = LinkPredictionDataset(train[:4], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "def collate_graphs(graphs):\n",
    "    collated_graph = {k: list() for k in graphs[0].keys()}\n",
    "    for g in graphs:\n",
    "        for k, v in g.items():\n",
    "            collated_graph[k].append(v)\n",
    "    \n",
    "    for k, v in collated_graph.items():\n",
    "        collated_graph[k] = dgl.batch(v)\n",
    "    return collated_graph\n",
    "\n",
    "loader = GraphDataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_pos_g': Graph(num_nodes=22, num_edges=36,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={}), 'train_neg_g': Graph(num_nodes=22, num_edges=36,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={}), 'test_pos_g': Graph(num_nodes=22, num_edges=8,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={}), 'test_neg_g': Graph(num_nodes=22, num_edges=8,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={}), 'train_g': Graph(num_nodes=22, num_edges=36,\n",
      "      ndata_schemes={'h': Scheme(shape=(128,), dtype=torch.float32)}\n",
      "      edata_schemes={'masked': Scheme(shape=(), dtype=torch.bool)})}\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "    return torch.nn.BCEWithLogitsLoss()(scores.float(), labels.float())\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).detach().numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).detach().numpy()\n",
    "    return roc_auc_score(labels, scores)\n",
    "\n",
    "\n",
    "\n",
    "class GNNLinkPredictionTrainer:\n",
    "    def __init__(self, model, predictor, args) -> None:\n",
    "        self.model = model\n",
    "        self.predictor = predictor\n",
    "        self.model.to(device)\n",
    "        self.optimizer = torch.optim.Adam(itertools.chain(model.parameters(), predictor.parameters()), lr=args.lr)\n",
    "\n",
    "        \n",
    "        self.edge2index = lambda g: torch.stack(list(g.edges())).contiguous()\n",
    "        self.args = args\n",
    "        print(\"GNN Trainer initialized.\")\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        self.model.train()\n",
    "        self.predictor.train()\n",
    "\n",
    "        epoch_loss, epoch_acc = 0, 0\n",
    "        for batch in dataloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            self.model.zero_grad()\n",
    "            self.predictor.zero_grad()\n",
    "            \n",
    "            h = self.get_logits(batch['train_g'])\n",
    "\n",
    "            pos_score = self.predictor(batch['train_pos_g'], h)\n",
    "            neg_score = self.predictor(batch['train_neg_g'], h)\n",
    "            loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += compute_auc(pos_score, neg_score)\n",
    "\n",
    "        epoch_loss /= len(dataloader)\n",
    "        epoch_acc /= len(dataloader)\n",
    "        print(f\"Epoch Train Loss: {epoch_loss} and Train Accuracy: {epoch_acc}\")\n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "\n",
    "    def test(self, dataloader):\n",
    "        self.model.eval()\n",
    "        self.predictor.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_loss, epoch_acc = 0, 0\n",
    "            for batch in dataloader:            \n",
    "                h = self.get_logits(batch['train_g'])\n",
    "\n",
    "                pos_score = self.predictor(batch['test_pos_g'], h)\n",
    "                neg_score = self.predictor(batch['test_neg_g'], h)\n",
    "                loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += compute_auc(pos_score, neg_score)\n",
    "\n",
    "            epoch_loss /= len(dataloader)\n",
    "            epoch_acc /= len(dataloader)\n",
    "            print(f\"Epoch Test Loss: {epoch_loss} and Test Accuracy: {epoch_acc}\")\n",
    "            return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "    def get_logits(self, g):\n",
    "        edge_index = self.edge2index(g).to(device)\n",
    "        x = g.ndata['h'].float()\n",
    "        h = self.model(x, edge_index)\n",
    "        return h\n",
    "\n",
    "\n",
    "    def get_prediction(self, h, g):\n",
    "        edge_index = self.edge2index(g).to(device)\n",
    "        out = self.predictor(h, edge_index)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def run_epochs(self, dataloader, num_epochs):\n",
    "        max_val_acc, max_train_acc = 0, 0\n",
    "        outputs = list()\n",
    "        for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        # for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = self.train(dataloader)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch} Train Loss: {train_loss}\")\n",
    "            \n",
    "            test_loss, test_acc = self.test(dataloader)\n",
    "\n",
    "            if test_acc > max_val_acc:\n",
    "                max_val_acc = test_acc\n",
    "                max_train_acc = train_acc\n",
    "                outputs.append({\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': train_loss,\n",
    "                    'test_loss': test_loss,\n",
    "                    'test_acc': test_acc\n",
    "                })\n",
    "\n",
    "            \n",
    "        \n",
    "        print(f\"Max Test Accuracy: {max_val_acc}\")\n",
    "        print(f\"Max Train Accuracy: {max_train_acc}\")\n",
    "        max_output = max(outputs, key=lambda x: x['test_acc'])\n",
    "        return max_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GNNModel, MLPPredictor\n",
    "\n",
    "gnn_model = GNNModel(\n",
    "    model_name='SAGEConv', \n",
    "    input_dim=128, \n",
    "    hidden_dim=256, \n",
    "    out_dim=256,\n",
    "    num_layers=2, \n",
    "    residual=True,\n",
    ")\n",
    "\n",
    "predictor = MLPPredictor(\n",
    "    h_feats=256,\n",
    "    num_layers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "lp_trainer = GNNLinkPredictionTrainer(gnn_model, predictor, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774295dff75b4c78826e5e72620bef12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Loss: 0.5127556025981903 and Train Accuracy: 0.894345137293505\n",
      "Epoch 0 Train Loss: 0.5127556025981903\n",
      "Epoch Test Loss: 0.5765130072832108 and Test Accuracy: 0.774725\n",
      "Epoch Train Loss: 0.37719155848026276 and Train Accuracy: 0.9280618372050121\n",
      "Epoch Test Loss: 0.6184224039316177 and Test Accuracy: 0.7858947681331747\n",
      "Epoch Train Loss: 0.38555996119976044 and Train Accuracy: 0.9286165557199211\n",
      "Epoch Test Loss: 0.6855111718177795 and Test Accuracy: 0.75985\n",
      "Epoch Train Loss: 0.2889115735888481 and Train Accuracy: 0.9468441212120338\n",
      "Epoch Test Loss: 0.7037681937217712 and Test Accuracy: 0.8326025564803805\n",
      "Epoch Train Loss: 0.3423616588115692 and Train Accuracy: 0.9424386711045365\n",
      "Epoch Test Loss: 0.7048476040363312 and Test Accuracy: 0.8343861474435196\n",
      "Max Test Accuracy: 0.8343861474435196\n",
      "Max Train Accuracy: 0.9424386711045365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 4,\n",
       " 'train_loss': 0.3423616588115692,\n",
       " 'test_loss': 0.7048476040363312,\n",
       " 'test_acc': 0.8343861474435196}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp_trainer.run_epochs(loader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CM-GNN-VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
