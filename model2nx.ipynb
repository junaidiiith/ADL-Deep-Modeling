{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10595\n"
     ]
    }
   ],
   "source": [
    "gen_db_file = '../datasets/ModelClassification/modelset/datasets/dataset.genmymodel/data/genmymodel.db'\n",
    "ecore_db_file = '../datasets/ModelClassification/modelset/datasets/dataset.ecore/data/ecore.db'\n",
    "db_files = [ecore_db_file, gen_db_file]\n",
    "\n",
    "def create_dataset_json():\n",
    "    count = 0\n",
    "    metadata = dict()\n",
    "    for file in db_files:\n",
    "        conn = sqlite3.connect(file)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT * FROM metadata;\")\n",
    "        rows = cursor.fetchall()\n",
    "        for row in rows:\n",
    "            try:\n",
    "                metadata[row[0]] = json.loads(row[2])\n",
    "            except:\n",
    "                metadata[row[0]] = dict()\n",
    "                count += 1\n",
    "        conn.close()\n",
    "    return metadata\n",
    "\n",
    "data = create_dataset_json()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10595\n",
      "repo-genmymodel-uml/data/40966914-1f3d-4f22-8334-5ef53c4cc64b.xmi {'type': ['behaviour'], 'category': ['petrinet'], 'tags': ['behaviour']}\n",
      "5475 5120\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(list(data.keys())[-1], data[list(data.keys())[0]])\n",
    "ecore_models = {k:v for k,v in data.items() if k.startswith('repo-ecore-all')}\n",
    "gen_models = {k:v for k,v in data.items() if k.startswith('repo-genmymodel-uml')}\n",
    "print(len(ecore_models), len(gen_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecore.resources import ResourceSet, URI\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "GENERALIZATION = 'generalization'\n",
    "ASSOCIATION = 'association'\n",
    "REFERENCE = 'reference'\n",
    "\n",
    "\n",
    "def get_attributes(classifier):\n",
    "    all_feats = set((feat.name, feat.eType.name) for feat in classifier.eAllStructuralFeatures() if type(feat).__name__ == 'EAttribute')\n",
    "    return list(all_feats)\n",
    "\n",
    "def get_model_root(file_name):\n",
    "    rset = ResourceSet()\n",
    "    resource = rset.get_resource(URI(file_name))\n",
    "    mm_root = resource.contents[0]\n",
    "    return mm_root\n",
    "\n",
    "\n",
    "def get_ecore_data(file_name):\n",
    "    rset = ResourceSet()\n",
    "    resource = rset.get_resource(URI(file_name))\n",
    "    mm_root = resource.contents[0]\n",
    "    references = list()\n",
    "    for classifier in mm_root.eClassifiers:\n",
    "        # print(classifier.name, get_features(classifier))\n",
    "        if type(classifier).__name__ == 'EClass':\n",
    "            references.append((classifier.name, get_attributes(classifier)))\n",
    "    super_types = list()\n",
    "    for classifier in mm_root.eClassifiers:\n",
    "        if type(classifier).__name__ == 'EClass':\n",
    "            for supertype in classifier.eAllSuperTypes():\n",
    "                super_types.append((classifier.name, supertype.name))\n",
    "    return references, super_types\n",
    "\n",
    "\n",
    "def create_nx_from_ecore(file_name):\n",
    "    try:\n",
    "        model_root = get_model_root(file_name)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    if type(model_root).__name__ != 'EPackage':\n",
    "        return None\n",
    "    nxg = nx.DiGraph()\n",
    "    for classifier in model_root.eClassifiers:\n",
    "        if type(classifier).__name__ == 'EClass':\n",
    "            if not nxg.has_node(classifier.name):\n",
    "                nxg.add_node(classifier.name, name=classifier.name, type='class')\n",
    "\n",
    "            classifier_attrs = set(feat.name for feat in classifier.eAllStructuralFeatures() if type(feat).__name__ == 'EAttribute')\n",
    "            nxg.nodes[classifier.name]['attrs'] = list(classifier_attrs)\n",
    "    \n",
    "    for classifier in model_root.eClassifiers:\n",
    "        if type(classifier).__name__ == 'EClass':\n",
    "            for supertype in classifier.eAllSuperTypes():\n",
    "                if not nxg.has_node(supertype.name):\n",
    "                    nxg.add_node(supertype.name, name=supertype.name, type='class')\n",
    "                nxg.add_edge(classifier.name, supertype.name, type=GENERALIZATION)\n",
    "            \n",
    "            for reference in classifier.eReferences:\n",
    "                try:\n",
    "                    if reference.eType is not None and not nxg.has_edge(classifier.name, reference.eType.name):\n",
    "                        nxg.add_edge(\n",
    "                            classifier.name, reference.eType.name, name=reference.name, \\\n",
    "                                type=REFERENCE if reference.containment else ASSOCIATION\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    # print(\"ref\", reference)\n",
    "                    # raise(e)\n",
    "                    pass\n",
    "        \n",
    "    return nxg\n",
    "\n",
    "\n",
    "def get_graphs_from_dir(models_metadata, dir=None):\n",
    "    graphs = list()\n",
    "    count = 0\n",
    "    models = models_metadata if dir is None else [os.path.join(dir, model) for model in models_metadata.keys()]\n",
    "    for model_file_name in tqdm(models):\n",
    "        try:\n",
    "            g = create_nx_from_ecore(model_file_name)\n",
    "            if g is not None:\n",
    "                graphs.append(g)\n",
    "        except Exception as e:\n",
    "            print(model_file_name)\n",
    "            count += 1\n",
    "    print(count)\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = '../datasets/ModelClassification/modelset/raw-data'\n",
    "graphs = get_graphs_from_dir(ecore_models, models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2677\n"
     ]
    }
   ],
   "source": [
    "filtered_graphs = [g for g in filter(lambda g: g.number_of_edges() >= 10, graphs)]\n",
    "print(len(filtered_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ecore_modelset_graphs.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ecore_dir = '/Users/junaid/Downloads/TUWien/Projects/CM-KB-Search-Project/MAR-Models-Repository/repo-github-ecore'\n",
    "\n",
    "## Recursively get all ecore files from the directory\n",
    "def get_all_ecore_files(dir):\n",
    "    ecore_files = list()\n",
    "    for root, _, filenames in os.walk(dir):\n",
    "        for filename in fnmatch.filter(filenames, '*.ecore'):\n",
    "            ecore_files.append(os.path.join(root, filename))\n",
    "    return ecore_files\n",
    "\n",
    "# all_ecore_files = get_all_ecore_files(all_ecore_dir)\n",
    "# print(len(all_ecore_files))\n",
    "\n",
    "with open('datasets/ecore_graph_pickles/all_ecore_files.pkl', 'rb') as f:\n",
    "    all_ecore_files = pickle.load(f)\n",
    "\n",
    "all_ecore_graphs = get_graphs_from_dir(all_ecore_files)\n",
    "\n",
    "with open('datasets/ecore_graph_pickles/all_ecore_graphs.pkl', 'rb') as f:\n",
    "    all_ecore_graphs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26393\n",
      "701\n"
     ]
    }
   ],
   "source": [
    "filtered_all_ecore_graphs = [g for g in filter(lambda g: g.number_of_edges() >= 10, all_ecore_graphs)]\n",
    "print(len(filtered_all_ecore_graphs))\n",
    "print(max([g.number_of_nodes() for g in filtered_all_ecore_graphs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph2str(g):\n",
    "    return str(g.edges())\n",
    "\n",
    "def remove_duplicates(graphs):\n",
    "    return list({graph2str(g):g for g in graphs}.values())\n",
    "\n",
    "def filter_graphs(graphs, min_edges=10):\n",
    "    return [g for g in filter(lambda g: g.number_of_edges() >= min_edges, graphs)]\n",
    "\n",
    "def clean_graph_set(graphs):\n",
    "    graphs = remove_duplicates(graphs)\n",
    "    graphs = filter_graphs(graphs)\n",
    "    return graphs\n",
    "\n",
    "def write_graphs_to_file(graphs, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(graphs, f)\n",
    "\n",
    "def read_graphs_from_file(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        graphs = pickle.load(f)\n",
    "    return graphs\n",
    "\n",
    "def write_clean_graphs_to_file(graphs, file_name):\n",
    "    graphs = clean_graph_set(graphs)\n",
    "    write_graphs_to_file(graphs, file_name)\n",
    "\n",
    "def read_clean_graphs_from_file(file_name):\n",
    "    graphs = read_graphs_from_file(file_name)\n",
    "    graphs = clean_graph_set(graphs)\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_graphs_to_file(graphs, 'datasets/ecore_graph_pickles/ecore_modelset_graphs.pkl')\n",
    "write_graphs_to_file(all_ecore_graphs, 'datasets/ecore_graph_pickles/all_ecore_graphs.pkl')\n",
    "write_clean_graphs_to_file(graphs, 'datasets/ecore_graph_pickles/ecore_modelset_graphs_clean.pkl')\n",
    "write_clean_graphs_to_file(all_ecore_graphs, 'datasets/ecore_graph_pickles/all_ecore_graphs_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_graphs = graphs + all_ecore_graphs\n",
    "write_graphs_to_file(combined_graphs, 'datasets/ecore_graph_pickles/combined_graphs.pkl')\n",
    "write_clean_graphs_to_file(combined_graphs, 'datasets/ecore_graph_pickles/combined_graphs_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6547\n"
     ]
    }
   ],
   "source": [
    "combined_clean_graphs = read_clean_graphs_from_file('datasets/ecore_graph_pickles/combined_graphs_clean.pkl')\n",
    "print(len(combined_clean_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/ecore_graph_pickles/combined_graph.pkl', 'rb') as f:\n",
    "    combined_graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "SEP = '->'\n",
    "remove_extra_spaces = lambda txt: re.sub(r'\\s+', ' ', txt.strip())\n",
    "\n",
    "edge_type_map = {\n",
    "    GENERALIZATION: 0,\n",
    "    ASSOCIATION: 1,\n",
    "    REFERENCE: 2,\n",
    "}\n",
    "\n",
    "def process_edge_for_string(graph, edge):\n",
    "    u, v = edge\n",
    "    edge_type = edge_type_map[graph.edges[edge]['type']]\n",
    "    edge_name = graph.edges[edge]['name'] if 'name' in graph.edges[u, v] else ''\n",
    "    # edge_str = (': ' + edge_name) if edge_name else ''\n",
    "    # edge_str = remove_extra_spaces(edge_str)\n",
    "    return remove_extra_spaces(edge_name), edge_type\n",
    "\n",
    "\n",
    "def process_node_for_string(graph, node, add_attrs=True):\n",
    "    assert graph.has_node(node) and 'name' in graph.nodes[node], \"Node not found in graph or node name not found in node\"\n",
    "        \n",
    "    node_name = graph.nodes[node]['name']\n",
    "    node_attrs_str = ''\n",
    "    if add_attrs:\n",
    "        node_attrs = graph.nodes[node]['attrs'] if 'attrs' in graph.nodes[node] else []\n",
    "        node_attrs_str = \"(\" + ((\"attributes=\" + ', '.join(node_attrs)) if len(node_attrs) else \"\") + \")\"\n",
    "        \n",
    "    node_str = node_name + node_attrs_str\n",
    "    node_str = remove_extra_spaces(node_str)\n",
    "    return node_str\n",
    "    \n",
    "\n",
    "def find_nodes_within_distance(graph, start_node, distance):\n",
    "    q, visited = deque(), dict()\n",
    "    q.append((start_node, 0))\n",
    "    \n",
    "    while q:\n",
    "        n, d = q.popleft()\n",
    "        if d <= distance:\n",
    "            visited[n] = d\n",
    "            neighbours = [\n",
    "                neighbor for node, neighbor in graph.edges(n) \\\n",
    "                    if neighbor != n and \\\n",
    "                        neighbor not in visited and \\\n",
    "                        graph.edges[node, neighbor]['type'] != GENERALIZATION\n",
    "                ]\n",
    "            for neighbour in neighbours:\n",
    "                if neighbour not in visited:\n",
    "                    q.append((neighbour, d + 1))\n",
    "    \n",
    "    sorted_list = sorted(visited.items(), key=lambda x: x[1])\n",
    "    return sorted_list\n",
    "\n",
    "def get_node_neighbours(graph, start_node, distance):\n",
    "    neighbours = find_nodes_within_distance(graph, start_node, distance)\n",
    "    max_distance = max(distance for _, distance in neighbours)\n",
    "    distance = min(distance, max_distance)\n",
    "    return [node for node, d in neighbours if d == distance]\n",
    "\n",
    "\n",
    "def get_triple_from_edge(g, edge, attrs=False):\n",
    "    u, v = edge\n",
    "    edge_str, edge_type = process_edge_for_string(g, edge)\n",
    "    u_string, v_string = process_node_for_string(g, u, add_attrs=attrs), process_node_for_string(g, v, add_attrs=attrs)\n",
    "    return ((u_string, edge_str, v_string), edge_type)\n",
    "\n",
    "\n",
    "def get_triples_from_edges(g, edges=None, attrs=False):\n",
    "    if edges is None:\n",
    "        edges = g.edges()\n",
    "    triples = []\n",
    "    for edge in edges:\n",
    "        triple = get_triple_from_edge(g, edge, attrs)\n",
    "        triples.append(triple)\n",
    "        \n",
    "    return triples\n",
    "\n",
    "\n",
    "def process_path_string(g, path, attrs=False):\n",
    "    edges = list(zip(path[:-1], path[1:]))\n",
    "    triples = get_triples_from_edges(g, edges, attrs)\n",
    "    Xs, ys = [\" \".join(t[0]) for t in triples], [t[1] for t in triples]\n",
    "\n",
    "    return Xs, ys\n",
    "\n",
    "\n",
    "def get_triples_from_node(g, n, distance=1, attrs=False):\n",
    "    triples = list()\n",
    "    node_neighbours = get_node_neighbours(g, n, distance)\n",
    "    for neighbour in node_neighbours:\n",
    "        paths = [p for p in nx.all_simple_paths(g, n, neighbour, cutoff=distance)]\n",
    "        for path in paths:\n",
    "            triples.append(process_path_string(g, path, attrs))\n",
    "    \n",
    "    return triples\n",
    "\n",
    "\n",
    "def get_graph_triples(g, distance=1, attrs=False):\n",
    "    triples = list()\n",
    "    for node in g.nodes():\n",
    "        triples += get_triples_from_node(g, node, distance, attrs)\n",
    "    return triples\n",
    "\n",
    "\n",
    "def get_triples(graphs, distance=1, attrs=False):\n",
    "    triples = []\n",
    "    for g in tqdm(graphs):\n",
    "        triples += get_graph_triples(g, distance, attrs)\n",
    "    return triples\n",
    "\n",
    "def remove_duplicate_triples(triples):\n",
    "    return list({t[0]:t for t in triples}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def mask_graph(g, mask_ratio=0.2):\n",
    "    for edge in g.edges():\n",
    "        g.edges[edge]['mask'] = False\n",
    "\n",
    "    num_edges = g.number_of_edges()\n",
    "    num_edges_to_mask = int(num_edges * mask_ratio)\n",
    "    edges_to_mask = np.random.choice(num_edges, num_edges_to_mask, replace=False)\n",
    "    for edge in edges_to_mask:\n",
    "        g.edges[edge]['mask'] = True\n",
    "    \n",
    "    return g\n",
    "\n",
    "def mask_graphs(graphs, mask_ratio=0.2):\n",
    "    masked_graphs = [mask_graph(g, mask_ratio) for g in graphs]\n",
    "    return masked_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0dcc0f29ab4500ac75fd9ba7a8c77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m distance:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr_flag:\n\u001b[0;32m----> 5\u001b[0m         triples \u001b[38;5;241m=\u001b[39m \u001b[43mget_triples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_clean_graphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         triples \u001b[38;5;241m=\u001b[39m remove_duplicate_triples(triples)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# break\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 105\u001b[0m, in \u001b[0;36mget_triples\u001b[0;34m(graphs, distance, attrs)\u001b[0m\n\u001b[1;32m    103\u001b[0m triples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m tqdm(graphs):\n\u001b[0;32m--> 105\u001b[0m     triples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mget_graph_triples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m triples\n",
      "Cell \u001b[0;32mIn[52], line 98\u001b[0m, in \u001b[0;36mget_graph_triples\u001b[0;34m(g, distance, attrs)\u001b[0m\n\u001b[1;32m     96\u001b[0m triples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m g\u001b[38;5;241m.\u001b[39mnodes():\n\u001b[0;32m---> 98\u001b[0m     triples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mget_triples_from_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m triples\n",
      "Cell \u001b[0;32mIn[52], line 86\u001b[0m, in \u001b[0;36mget_triples_from_node\u001b[0;34m(g, n, distance, attrs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_triples_from_node\u001b[39m(g, n, distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     85\u001b[0m     triples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m---> 86\u001b[0m     node_neighbours \u001b[38;5;241m=\u001b[39m \u001b[43mget_node_neighbours\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m neighbour \u001b[38;5;129;01min\u001b[39;00m node_neighbours:\n\u001b[1;32m     88\u001b[0m         paths \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mall_simple_paths(g, n, neighbour, cutoff\u001b[38;5;241m=\u001b[39mdistance)]\n",
      "Cell \u001b[0;32mIn[52], line 52\u001b[0m, in \u001b[0;36mget_node_neighbours\u001b[0;34m(graph, start_node, distance)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_node_neighbours\u001b[39m(graph, start_node, distance):\n\u001b[0;32m---> 52\u001b[0m     neighbours \u001b[38;5;241m=\u001b[39m \u001b[43mfind_nodes_within_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     max_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(distance \u001b[38;5;28;01mfor\u001b[39;00m _, distance \u001b[38;5;129;01min\u001b[39;00m neighbours)\n\u001b[1;32m     54\u001b[0m     distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(distance, max_distance)\n",
      "Cell \u001b[0;32mIn[52], line 43\u001b[0m, in \u001b[0;36mfind_nodes_within_distance\u001b[0;34m(graph, start_node, distance)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m distance:\n\u001b[1;32m     42\u001b[0m     visited[n] \u001b[38;5;241m=\u001b[39m d\n\u001b[0;32m---> 43\u001b[0m     neighbours \u001b[38;5;241m=\u001b[39m [neighbor \u001b[38;5;28;01mfor\u001b[39;00m neighbor \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mneighbors(n) \u001b[38;5;28;01mif\u001b[39;00m neighbor \u001b[38;5;241m!=\u001b[39m n \u001b[38;5;129;01mand\u001b[39;00m neighbor \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited]\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m neighbour \u001b[38;5;129;01min\u001b[39;00m neighbours:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m neighbour \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited:\n",
      "Cell \u001b[0;32mIn[52], line 43\u001b[0m, in \u001b[0;36mfind_nodes_within_distance\u001b[0;34m(graph, start_node, distance)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m distance:\n\u001b[1;32m     42\u001b[0m     visited[n] \u001b[38;5;241m=\u001b[39m d\n\u001b[0;32m---> 43\u001b[0m     neighbours \u001b[38;5;241m=\u001b[39m [neighbor \u001b[38;5;28;01mfor\u001b[39;00m neighbor \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mneighbors(n) \u001b[38;5;28;01mif\u001b[39;00m neighbor \u001b[38;5;241m!=\u001b[39m n \u001b[38;5;129;01mand\u001b[39;00m neighbor \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited]\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m neighbour \u001b[38;5;129;01min\u001b[39;00m neighbours:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m neighbour \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited:\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CM-GNN-VENV/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CM-GNN-VENV/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "distance = [1, 2, 3]\n",
    "attr_flag = [False, True]\n",
    "for d in distance:\n",
    "    for a in attr_flag:\n",
    "        triples = get_triples(combined_clean_graphs, distance=d, attrs=a)\n",
    "        triples = remove_duplicate_triples(triples)\n",
    "    #     break\n",
    "    # break\n",
    "        print(\"Total triples:\", len(triples))\n",
    "        \n",
    "        with open(f'datasets/ecore_graph_pickles/combined_graphs_triples_d{d}_attr{a}.json', 'w') as f:\n",
    "            json.dump(triples, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_graphs(graphs):\n",
    "    combined_graph = nx.DiGraph()\n",
    "    for g in tqdm(graphs):\n",
    "        for edge in g.edges():\n",
    "            u, v = edge\n",
    "            if not combined_graph.has_node(u):\n",
    "                combined_graph.add_node(u, **g.nodes[u])\n",
    "            if not combined_graph.has_node(v):\n",
    "                combined_graph.add_node(v, **g.nodes[v])\n",
    "                \n",
    "            if not combined_graph.has_edge(*edge):\n",
    "                combined_graph.add_edge(*edge, **g.edges[edge])\n",
    "    return combined_graph\n",
    "\n",
    "combined_graph = combine_graphs(combined_clean_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('datasets/ecore_graph_pickles/combined_graphs_triples_d3_attrFalse.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edges, classes in tqdm(data):\n",
    "    edges = edges.split(f' {SEP} ')\n",
    "    for i, j in zip(edges, classes):\n",
    "        try:\n",
    "            if j == 0:\n",
    "                assert len(i.split()) == 2\n",
    "            else:\n",
    "                assert len(i.split()) == 3\n",
    "        except AssertionError as e:\n",
    "            print(i, j)\n",
    "            # raise(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3400bab4754834a38ff3ee6f03241a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9726865aa649e7857a85e1e3c0fdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "super_types = list(set([edge[1] for g in tqdm(combined_clean_graphs) for n in g.nodes() for edge in g.edges(n) if g.edges[edge]['type'] == 'generalization']))\n",
    "entities = list(set([n for g in tqdm(combined_clean_graphs) for n in g.nodes()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13234, 62788)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(super_types), len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2020246db85e4df9b4ae6a94b6302abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "39397"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_with_super_types = dict()\n",
    "for g in tqdm(combined_clean_graphs):\n",
    "    for n in g.nodes():\n",
    "        gen_edges = [edge[1] for edge in g.edges(n) if g.edges[edge]['type'] == 'generalization']\n",
    "        other_edges = [edge[1] for edge in g.edges(n) if g.edges[edge]['type'] != 'generalization']\n",
    "        if len(gen_edges) > 1:\n",
    "            key_str = f\"{n} {SEP} {' '.join(gen_edges)}\"\n",
    "            nodes_with_super_types[key_str] = (n, gen_edges)\n",
    "\n",
    "len(nodes_with_super_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3439a4b802cf4beeb28839d697259806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [len(g.edges(n)) for g in tqdm(combined_clean_graphs) for n in g.nodes()]\n",
    "max(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(l)\n",
    "sorted(c.items(), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('supertypes.txt', 'w') as f:\n",
    "    for _, v in nodes_with_super_types.items():\n",
    "        f.write(f'{v[0]} -> {v[1]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac82c37b2474a8c9ff77b5c05bda23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "77972"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_nodes = [\n",
    "    n for g in tqdm(combined_clean_graphs) for n in g.nodes() \\\n",
    "        if len([edge for edge in g.edges(n) if g.edges[edge]['type'] != 'generalization'])\n",
    "]\n",
    "\n",
    "len(relevant_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32291"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(relevant_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d9695f95d14b61ba0dfc39dbce0172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109715\n"
     ]
    }
   ],
   "source": [
    "node_triples = set()\n",
    "\n",
    "for g in tqdm(combined_clean_graphs):\n",
    "    for n in g.nodes():\n",
    "        super_type_nodes = [edge[1] for edge in g.edges(n) if g.edges[edge]['type'] == 'generalization' and len(edge[1].strip())]\n",
    "\n",
    "        if 'NamedElement' in super_type_nodes:\n",
    "            super_type_nodes.remove('NamedElement')\n",
    "\n",
    "        reference_nodes = [edge[1] for edge in g.edges(n) if g.edges[edge]['type'] != 'generalization' and len(edge[1].strip())]\n",
    "        if not len(reference_nodes):\n",
    "            continue\n",
    "\n",
    "        selected_super_types = random.sample(super_type_nodes, min(5, len(super_type_nodes)))\n",
    "\n",
    "        node_references = [edge[1] for edge in g.edges(n) if g.edges[edge]['type'] != 'generalization']\n",
    "        for node_reference in node_references:\n",
    "            edge_name = g.edges[n, node_reference]['name'] if 'name' in g.edges[n, node_reference] else ''\n",
    "            node_triples.add((n, edge_name, node_reference, \", \".join(selected_super_types)))\n",
    "\n",
    "node_triples = list(node_triples)\n",
    "print(len(node_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8366\n"
     ]
    }
   ],
   "source": [
    "all_super_types = set([st for t in node_triples for st in t[3].split(', ') if len(st.strip())])\n",
    "print(len(all_super_types))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42940\n"
     ]
    }
   ],
   "source": [
    "all_entities = set([t[0] for t in node_triples] + [t[2] for t in node_triples])\n",
    "print(len(all_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98743 10972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(node_triples, test_size=0.1, random_state=42)\n",
    "\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entities, train_super_types = set([t[0] for t in train] + [t[2] for t in train]), set([st for t in train for st in t[3].split(', ')])\n",
    "# val_entities, val_super_types = set([t[0] for t in val] + [t[2] for t in val]), set([st for t in val for st in t[3].split(', ')])\n",
    "test_entities, test_super_types = set([t[0] for t in test] + [t[2] for t in test]), set([st for t in test for st in t[3].split(', ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41336 11463\n",
      "1604\n"
     ]
    }
   ],
   "source": [
    "print(len(train_entities), len(test_entities))\n",
    "print(sum([1 for v in test_entities if v not in train_entities]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10117\n"
     ]
    }
   ],
   "source": [
    "test_seen = list([v for v in test if v[0] in train_entities])\n",
    "\n",
    "print(len(test_seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "855\n"
     ]
    }
   ],
   "source": [
    "test_unseen = list([v for v in test if v[0] not in train_entities])\n",
    "\n",
    "print(len(test_unseen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_entities = list(set([t[0] for t in train] + [t[0] for t in test_seen]))\n",
    "\n",
    "unseen_entities = list(set([t[0] for t in test_unseen]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_super_types = list(set([st for t in train for st in t[3].split(', ')] + [st for t in test_seen for st in t[3].split(', ')]))\n",
    "unseen_super_types = list(set([st for t in test_unseen for st in t[3].split(', ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'train': train,\n",
    "    'test': test,\n",
    "    'test_seen': test_seen,\n",
    "    'test_unseen': test_unseen,\n",
    "    'seen_entities': seen_entities,\n",
    "    'unseen_entities': unseen_entities,\n",
    "    'seen_super_types': seen_super_types,\n",
    "    'unseen_super_types': unseen_super_types,\n",
    "}\n",
    "\n",
    "with open('datasets/ecore_graph_pickles/ecore_node_triples.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_super_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10972, 5729)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test), sum([1 for i in test if len(i[-1].strip())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {v:k for k,v in enumerate(seen_entities)}\n",
    "labels = [label_map[i[0]] for i in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp_map = {v:k for k,v in enumerate(seen_super_types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "\n",
    "\n",
    "def get_stereotype_labels(triples, stereotype_map, multi_label=True):\n",
    "    stp_labels = [[stereotype_map[j] for j in i[3].split(', ') if len(j.strip())] for i in triples]\n",
    "    if not multi_label:\n",
    "        le = LabelEncoder()\n",
    "        stp_labels = le.fit_transform([i[0] if len(i) else -1 for i in stp_labels])\n",
    "\n",
    "        stp_labels = torch.from_numpy(stp_labels)\n",
    "    else:\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        stp_labels = torch.from_numpy(mlb.fit_transform(stp_labels))\n",
    "        \n",
    "    return stp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6301)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(stp_map))\n",
    "stp_labels = get_stereotype_labels(train, stp_map, multi_label=False)\n",
    "max(stp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, triples, entity_map, stereotype_map, tokenizer, multi_label=True):\n",
    "        self.labels = torch.from_numpy(np.array([entity_map[t[0]] for t in triples]))\n",
    "        self.stereotype_labels = get_stereotype_labels(triples, stereotype_map, multi_label)\n",
    "        \n",
    "        triples = [f'{tokenizer.mask_token} {t[1]} {t[2]}' for t in triples]\n",
    "        self.tokenized = tokenizer(triples, padding=True, return_tensors='pt')\n",
    "\n",
    "        self.entity_map = entity_map\n",
    "        self.stereotype_map = stereotype_map\n",
    "\n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        return torch.unique(self.labels).shape[0]\n",
    "\n",
    "    @property\n",
    "    def num_stereotype_labels(self):\n",
    "        if len(self.stereotype_labels.shape) == 1:\n",
    "            return torch.unique(self.stereotype_labels).shape[0]\n",
    "        return self.stereotype_labels.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = {\n",
    "            'input_ids': self.tokenized['input_ids'][idx],\n",
    "            'attention_mask': self.tokenized['attention_mask'][idx],\n",
    "            'labels': self.labels[idx],\n",
    "        }\n",
    "        entity_label = self.labels[idx]\n",
    "        stereotype_label = self.stereotype_labels[idx]\n",
    "\n",
    "        return inputs, entity_label, stereotype_label\n",
    "\n",
    "label_map = {v:k for k,v in enumerate(seen_entities)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereotype_map = {v:k for k,v in enumerate(seen_super_types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_label = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TripleDataset(data['train'], label_map, stereotype_map, tokenizer, multi_label)\n",
    "test_dataset = TripleDataset(data['test_seen'], label_map, stereotype_map, tokenizer, multi_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleClassifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Classifier that first uses the model to get the pooled output\n",
    "        Then applies a linear layer to get the logits for the entity classification\n",
    "        Then applies another linear layer to get the logits for the stereotype classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_labels, num_stp_labels, model_name='xlm-roberta-base'):\n",
    "        super(TripleClassifier, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.num_labels = num_labels\n",
    "        self.num_stp_labels = num_stp_labels\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.linear = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)\n",
    "        self.stp_linear = torch.nn.Linear(self.model.config.hidden_size, self.num_stp_labels)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        stp_logits = self.stp_linear(pooled_output)\n",
    "        return self.softmax(logits), self.softmax(stp_logits)\n",
    "    \n",
    "\n",
    "    def get_entity_loss(self, logits, labels):\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def get_stereotype_loss(self, stp_logits, stp_labels):\n",
    "        \"\"\"\n",
    "            stp_logits: (batch_size, num_stp_labels)\n",
    "            stp_labels: (batch_size, num_stp_labels)\n",
    "            This method calculates the loss for the stereotype classification such that,\n",
    "            if stp_labels shape is (batch_size, num_stp_labels), then the loss is calculated using cross entropy loss\n",
    "            else if stp_labels shape is (batch_size, num_stp_labels, k), then the loss is calculated using binary cross entropy loss\n",
    "        \"\"\"\n",
    "\n",
    "        if len(stp_labels.shape) == 1:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(stp_logits, stp_labels)\n",
    "        else:\n",
    "            loss_fct = torch.nn.BCELoss()\n",
    "            loss = loss_fct(stp_logits.float(), stp_labels.float())\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def get_loss(self, logits, stp_logits, labels, stp_labels, alpha=0.5):\n",
    "        entity_loss = self.get_entity_loss(logits, labels)\n",
    "        stp_loss = self.get_stereotype_loss(stp_logits, stp_labels)\n",
    "        loss = alpha * entity_loss + (1 - alpha) * stp_loss\n",
    "        return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TripleClassifier(\n",
       "  (model): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): XLMRobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=30640, bias=True)\n",
       "  (stp_linear): Linear(in_features=768, out_features=8181, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TripleClassifier(train_dataset.num_labels, train_dataset.num_stereotype_labels, model_name='xlm-roberta-base')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 38]) torch.Size([4, 38]) torch.Size([4]) torch.Size([4, 8181])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/CM-GNN-VENV/lib/python3.8/queue.py:167\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[320], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m logits, stp_logits \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_loss(logits, stp_logits[stp_mask], entity_labels, stereotype_labels[stp_mask])\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[320], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m logits, stp_logits \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_loss(logits, stp_logits[stp_mask], entity_labels, stereotype_labels[stp_mask])\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CM-GNN-VENV/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CM-GNN-VENV/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2105\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m in_main_thread \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgui_in_use:\n\u001b[1;32m   2102\u001b[0m         \u001b[38;5;66;03m# call input hooks if only GUI is in use\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m-> 2105\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_internal_commands\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CM-GNN-VENV/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:1753\u001b[0m, in \u001b[0;36mPyDB.process_internal_commands\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1753\u001b[0m         int_cmd \u001b[38;5;241m=\u001b[39m \u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmpl_hooks_in_debug_console \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(int_cmd, InternalConsoleExec) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgui_in_use:\n\u001b[1;32m   1756\u001b[0m             \u001b[38;5;66;03m# add import hooks for matplotlib patches if only debug console was started\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CM-GNN-VENV/lib/python3.8/queue.py:167\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch, entity_labels, stereotype_labels in train_dataloader:\n",
    "    print(batch['input_ids'].shape, batch['attention_mask'].shape, entity_labels.shape, stereotype_labels.shape)\n",
    "    ### Start training loop\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    entity_labels = entity_labels.to(device)\n",
    "    stereotype_labels = stereotype_labels.to(device)\n",
    "\n",
    "    if stereotype_labels.shape[-1] == 1:\n",
    "        stp_mask = (stereotype_labels != -1)\n",
    "    \n",
    "\n",
    "    logits, stp_logits = model(input_ids, attention_mask)\n",
    "    loss = model.get_loss(logits, stp_logits[stp_mask], entity_labels, stereotype_labels[stp_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CM-GNN-VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
