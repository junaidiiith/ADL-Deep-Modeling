{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import networkx as nx\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROOT_TAG = '<MODEL>'\n",
    "CLS_TAG = '<CLS>'\n",
    "CLS_NAME_TAG = '<NAME>'\n",
    "ATTRS_TAG = '<ATTRS>'\n",
    "ASSOCS_TAG = '<ASSOCS>'\n",
    "OPEN_CHAR = '('\n",
    "CLOSE_CHAR = ')'\n",
    "special_tags = [ROOT_TAG, CLS_TAG, CLS_NAME_TAG, ATTRS_TAG, ASSOCS_TAG, OPEN_CHAR, CLOSE_CHAR]\n",
    "\n",
    "ecore_keys = ['ecore:EPackage', 'ecore:EClass', 'ecore:EEnum', 'EPackage']\n",
    "\n",
    "ecore_mapping = {\n",
    "    'ecore:EClass': 'eStructuralFeatures',\n",
    "    'ecore:EEnum': 'eLiterals',\n",
    "}\n",
    "\n",
    "OTHER_WITHIN_CLASSES = \"other classes within package\"\n",
    "OTHER_ACROSS_CLASSES = \"other classes across packages\"\n",
    "OTHER_PACKAGES = \"other packages\"\n",
    "\n",
    "\n",
    "TRAIN_MODELS = 'WeyssowPMC/train/repo_ecore_all.txt'\n",
    "TEST_MODELS_ITERATIVE = 'WeyssowPMC/test/test_iterative_construction.txt'\n",
    "TEST_MODELS_FULL_CONTEXT = 'WeyssowPMC/test/test_probing_full_context.txt'\n",
    "TEST_MODELS_LOCAL = 'WeyssowPMC/test/test_probing_local_context.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    " s = \"( <MODEL> ( <CLS> ( <NAME> GeneralizableElement ) ) ( <CLS> ( <NAME> <mask> ) ) )\"\n",
    "\"\"\"\n",
    "\n",
    "def get_model_from_tree_text(text):\n",
    "    current_special = None\n",
    "    i = 0\n",
    "    classes = dict()\n",
    "    l = text.split()\n",
    "    while i < len(l):\n",
    "        current_token = l[i]\n",
    "        \n",
    "        if current_token == CLS_NAME_TAG:\n",
    "            current_special = CLS_TAG\n",
    "            current_class = l[i+1]\n",
    "            classes[current_class] = {'assocs': list(), 'attrs': list()}\n",
    "            i += 1\n",
    "        elif current_token == ASSOCS_TAG:\n",
    "            current_special = ASSOCS_TAG\n",
    "        elif current_token == ATTRS_TAG:\n",
    "            current_special = ATTRS_TAG\n",
    "        elif current_token not in special_tags:\n",
    "            x, y = current_token, l[i+1]\n",
    "            if current_special == ASSOCS_TAG:\n",
    "                classes[current_class]['assocs'].append((x, y))\n",
    "            else:\n",
    "                classes[current_class]['attrs'].append((x, y))\n",
    "            i += 1    \n",
    "        i += 1\n",
    "    return classes\n",
    "\n",
    "\n",
    "def model_to_graph(graph, model_dict):\n",
    "    for cls in model_dict:\n",
    "        attrs = model_dict[cls]['attrs']\n",
    "        if cls not in graph:\n",
    "            graph.add_node(cls, type='class')\n",
    "        graph.nodes[cls]['attributes'] = [x for x in attrs]\n",
    "\n",
    "    for cls in model_dict:\n",
    "        assocs = model_dict[cls]['assocs']\n",
    "        for assoc in assocs:\n",
    "            assoc_cls, assoc_name = assoc\n",
    "            if not graph.has_edge(cls, assoc_cls):\n",
    "                graph.add_edge(cls, assoc_cls, name=assoc_name, type='association')\n",
    "\n",
    "s = \"( <MODEL> ( <CLS> ( <NAME> GeneralizableElement ) ) ( <CLS> ( <NAME> <mask> ) ) )\"\n",
    "graph = nx.DiGraph()\n",
    "model_to_graph(graph, get_model_from_tree_text(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_consolidated_graph(texts):\n",
    "    graph = nx.DiGraph()\n",
    "    for text in tqdm(texts):\n",
    "        model_dict = get_model_from_tree_text(text)\n",
    "        model_to_graph(graph, model_dict)\n",
    "    return graph\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521d71b471a44573ae25ee4af0350ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23327 49879\n"
     ]
    }
   ],
   "source": [
    "train_texts = open(TRAIN_MODELS).read().split('\\n')\n",
    "train_graph = generate_consolidated_graph(train_texts)\n",
    "print(train_graph.number_of_nodes(), train_graph.number_of_edges())\n",
    "nx.write_gpickle(train_graph, 'pmc_data/pmc_train_graph.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23327, 49879)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph.number_of_nodes(), train_graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_full_context = open(TEST_MODELS_FULL_CONTEXT).read().split('\\n')\n",
    "\n",
    "test_context_local = [(\";\".join(i.split(';')[:-1]), i.split(';')[-1]) for i in open(TEST_MODELS_LOCAL).read().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterative_construction = list()\n",
    "for i in open(TEST_MODELS_ITERATIVE).read().split('\\n'):\n",
    "    iterative_model = list()\n",
    "    if len(i) > 0:\n",
    "        model_str, iteration, element_type, element_name = i.split(';')\n",
    "        iterative_model.append((model_str, iteration, element_type, element_name))\n",
    "    test_iterative_construction.append(iterative_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_text_to_graph(model_text):\n",
    "    model_dict = get_model_from_tree_text(model_text)\n",
    "    graph = nx.DiGraph()\n",
    "    model_to_graph(graph, model_dict)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "MASK = \"<mask>\"\n",
    "\n",
    "def create_model_text_to_string(model_text, mask=-1):\n",
    "    graph = model_text_to_graph(model_text)\n",
    "    return create_graph_to_string(graph, mask)\n",
    "\n",
    "\n",
    "def create_graph_to_string(graph, mask=-1, mask_type='class'):\n",
    "    node2str = dict()\n",
    "    for node in graph.nodes:\n",
    "        node_str = f\"{node}\"\n",
    "        if 'attributes' in graph.nodes[node]:\n",
    "            attrs_str = (\"attributes=\" + \", \".join([f\"{attr[1]}: {attr[0]}\" for attr in graph.nodes[node]['attributes']]) if len(graph.nodes[node]['attributes']) else \"\")\n",
    "        else:\n",
    "            attrs_str = \"\"\n",
    "        # print(node_str)\n",
    "        node2str[node] = node_str + '(' + attrs_str + ')'\n",
    "    \n",
    "    if isinstance(mask, int) and mask > 0:\n",
    "        try:\n",
    "            masked_nodes = random.sample(list(graph.nodes), mask)\n",
    "            for node in masked_nodes:\n",
    "                node2str[node] = MASK\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Mask value {mask} is greater than number of nodes {len(graph.nodes)}\")\n",
    "\n",
    "    elif isinstance(mask, float) and mask > 0:\n",
    "        mask = max(int(mask * len(graph.nodes)), 1)\n",
    "        try:\n",
    "            masked_nodes = random.sample(list(graph.nodes), mask)\n",
    "            for node in masked_nodes:\n",
    "                node2str[node] = MASK\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Mask value {mask} is greater than number of nodes {len(graph.nodes)}\")\n",
    "        \n",
    "    node_rep = lambda n: f\"Class {n if node2str[n] != MASK else MASK}\"\n",
    "    edge_rep = lambda edge, graph: {graph.edges[edge]['name'] if (node2str[edge[0]] != MASK and node2str[edge[1]] != MASK) else MASK}\n",
    "        \n",
    "    node_strs = \"\\n\".join([f\"Class {node2str[node]}\" for node in graph.nodes])\n",
    "    edge_strs = \"\\n\".join([f\"{node_rep(edge[0])} association {edge_rep(edge, graph)} association Class {node_rep(edge[1])}\" for edge in graph.edges])\n",
    "    if mask <= 0:\n",
    "        return \"Nodes: \\n\" + node_strs + '\\nRelations: \\n' + edge_strs\n",
    "    return \"Nodes: \\n\" + node_strs + '\\nRelations: \\n' + edge_strs, masked_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_model_text_to_string(train_texts[89], mask=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list()\n",
    "num_samples = 20\n",
    "for train_text in tqdm(train_texts):\n",
    "    model_str = create_model_text_to_string(train_text)\n",
    "    texts.append(model_str)\n",
    "\n",
    "with open('pmc_data/pmc_train_text.json', 'w') as f:\n",
    "    json.dump(texts, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_values = [1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "for mask_value in mask_values:\n",
    "    texts = list()\n",
    "    for test_text in tqdm(test_text_full_context):\n",
    "        if test_text.strip() == \"\":\n",
    "            continue\n",
    "        model_str, masked_nodes = create_model_text_to_string(test_text, mask=mask_value)\n",
    "        texts.append((model_str, masked_nodes))\n",
    "    with open(f\"pmc_data/pmc_test_full_context_mask_{mask_value}.json\", 'w') as f:\n",
    "        json.dump(texts, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_construction_models = list()\n",
    "for iterative_model in tqdm(test_iterative_construction):\n",
    "\n",
    "    if len(iterative_model) == 0:\n",
    "        continue\n",
    "    iterative_texts = list()\n",
    "    for iterative_text in iterative_model:\n",
    "        model, iteration, element_type, element_name = iterative_text\n",
    "        model_str = create_model_text_to_string(model, mask=element_name)\n",
    "        iterative_texts.append((model_str, iteration, element_type, element_name))\n",
    "    iterative_construction_models.append(iterative_texts)\n",
    "\n",
    "with open(f\"pmc_data/pmc_test_iterative_construction.json\", 'w') as f:\n",
    "    json.dump(iterative_construction_models, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"( <MODEL> ( <CLS> ( <NAME> MEDiagram ) ( <ATTRS> ( EString diagramLayout ) ) ( <ASSOCS> ( Annotation annotations ) ( Attachment attachments ) ( LeafSection incomingDocumentReferences ) ( LeafSection leafSection ) ( StereotypeInstance appliedStereotypeInstances ) ( Comment comments ) ( UnicaseModelElement referringModelElements ) ( UnicaseModelElement elements ) ( UnicaseModelElement newElements ) ) ) ( <CLS> ( <NAME> ClassDiagram ) ( <ASSOCS> ( Annotation annotations ) ( Attachment attachments ) ( LeafSection incomingDocumentReferences ) ( LeafSection leafSection ) ( StereotypeInstance appliedStereotypeInstances ) ( Comment comments ) ( UnicaseModelElement referringModelElements ) ( UnicaseModelElement elements ) ( UnicaseModelElement newElements ) ) ) ( <CLS> ( <NAME> UseCaseDiagram ) ( <ASSOCS> ( Annotation annotations ) ( Attachment attachments ) ( LeafSection incomingDocumentReferences ) ( LeafSection leafSection ) ( StereotypeInstance appliedStereotypeInstances ) ( Comment comments ) ( UnicaseModelElement referringModelElements ) ( UnicaseModelElement elements ) ( UnicaseModelElement newElements ) ) ) ( <CLS> ( <NAME> ComponentDiagram ) ( <ASSOCS> ( Annotation annotations ) ( Attachment attachments ) ( LeafSection incomingDocumentReferences ) ( LeafSection leafSection ) ( StereotypeInstance appliedStereotypeInstances ) ( Comment comments ) ( UnicaseModelElement referringModelElements ) ( UnicaseModelElement elements ) ( UnicaseModelElement newElements ) ) ) ( <CLS> ( <NAME> StateDiagram ) ( <ASSOCS> ( Annotation annotations ) ( Attachment attachments ) ( LeafSection incomingDocumentReferences ) ( LeafSection leafSection ) ( StereotypeInstance appliedStereotypeInstances ) ( Comment comments ) ( UnicaseModelElement referringModelElements ) ( UnicaseModelElement elements ) ( UnicaseModelElement newElements ) ) ) ( <CLS> ( <NAME> ActivityDiagram ) ( <ASSOCS> ( Annotation annotations ) ( Attachment attachments ) ( LeafSection incomingDocumentReferences ) ( LeafSection leafSection ) ( StereotypeInstance appliedStereotypeInstances ) ( Comment comments ) ( UnicaseModelElement referringModelElements ) ( UnicaseModelElement elements ) ( UnicaseModelElement newElements ) ) ) ( <CLS> ( <NAME> WorkItemDiagram ) ( <ASSOCS> ( Annotation annotations ) ( Attachment attachments ) ( LeafSection incomingDocumentReferences ) ( LeafSection leafSection ) ( StereotypeInstance appliedStereotypeInstances ) ( Comment comments ) ( UnicaseModelElement referringModelElements ) ( UnicaseModelElement elements ) ( UnicaseModelElement newElements ) ) ) )\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = nx.read_gpickle('pmc_data/pmc_train_graph.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_map(graph):\n",
    "    node2str = dict()\n",
    "    for node in graph.nodes:\n",
    "        node_str = f\"{node}\"\n",
    "        if 'attributes' in graph.nodes[node]:\n",
    "            attrs_str = (\"attributes=\" + \", \".join([f\"{attr[1]}: {attr[0]}\" for attr in graph.nodes[node]['attributes']]) if len(graph.nodes[node]['attributes']) else \"\")\n",
    "        else:\n",
    "            attrs_str = \"\"\n",
    "        # print(node_str)\n",
    "        node2str[node] = node_str + '(' + attrs_str + ')'\n",
    "    return node2str\n",
    "\n",
    "\n",
    "def get_attribute_docs(graph, node2str, with_classes_context=False, with_attributes_context=False):\n",
    "    node_name = lambda node: f\"Class {node2str[node]}\" if with_attributes_context and node in node2str else f\"Class {node}\"\n",
    "    attribute_docs = list()\n",
    "    for node in graph.nodes:\n",
    "        if 'attributes' in graph.nodes[node] and len(graph.nodes[node]['attributes']) > 0:\n",
    "            context = \", \".join([f\"{node_name(neighbor)}\" for neighbor in graph.neighbors(node)]) if with_classes_context else \"\"\n",
    "            node_str = f\"{node + (' ' + context if with_classes_context else '')}\"\n",
    "            for x in graph.nodes[node]['attributes']:\n",
    "                attribute_docs.append((node_str, x[1]))\n",
    "\n",
    "    return attribute_docs\n",
    "\n",
    "\n",
    "def get_association_docs(graph, node2str, with_classes_context=False, with_attributes_context=False):\n",
    "    node_name = lambda node: f\"Class {node2str[node]}\" if with_attributes_context and node in node2str else f\"Class {node}\"\n",
    "\n",
    "    association_docs = list()\n",
    "    for edge in graph.edges:\n",
    "        c1, c2 = edge\n",
    "        association_name, _ = graph.edges[edge]['name'], graph.edges[edge]['type']\n",
    "        context = \", \".join([f\"{node_name(neighbor)}\" for neighbor in graph.neighbors(edge[0])]) if with_classes_context else \"\"\n",
    "        \n",
    "        association_docs.append((f\"{node_name(c1) + (' ' + context + ' ' if with_classes_context else ' ') + node_name(c2)}\", association_name))\n",
    "        \n",
    "    return association_docs\n",
    "\n",
    "\n",
    "def get_class_docs(graph, node2str, with_attributes_context=False):\n",
    "    node_name = lambda node: f\"Class {node2str[node]}\" if with_attributes_context and node in node2str else f\"Class {node}\"\n",
    "    class_docs = list()\n",
    "    for node in graph.nodes:\n",
    "        context = \", \".join([f\"{node_name(neighbor)}\" for neighbor in graph.neighbors(node)])\n",
    "        if context == \"\":\n",
    "            context = \", \".join([f\"{node_name(neighbor)}\" for neighbor in graph.nodes if neighbor != node])\n",
    "\n",
    "        class_docs.append((f\"{context}\", node))\n",
    "    return class_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def create_dataset(texts, split='train'):\n",
    "    attribute_context = [0, 1]\n",
    "    class_context = [0, 1]\n",
    "\n",
    "    configs = [c for c in itertools.product(attribute_context, class_context)]\n",
    "\n",
    "    configs_dataset = {c: {'class': list(), 'associations': list(), 'attributes': list()} for c in configs}\n",
    "\n",
    "    \n",
    "    for train_text in tqdm(texts):\n",
    "        if train_text.strip() == \"\":\n",
    "                continue\n",
    "        graph = model_text_to_graph(train_text)\n",
    "        node_str = get_node_map(train_graph)\n",
    "        \n",
    "        for config in configs:\n",
    "            attribute_context, class_context = config\n",
    "            attribute_docs = get_attribute_docs(graph, node_str, with_classes_context=class_context, with_attributes_context=attribute_context)\n",
    "            association_docs = get_association_docs(graph, node_str, with_classes_context=class_context, with_attributes_context=attribute_context)\n",
    "            class_docs = get_class_docs(graph, node_str, with_attributes_context=attribute_context)\n",
    "            configs_dataset[config]['class'].extend(class_docs)\n",
    "            configs_dataset[config]['associations'].extend(association_docs)\n",
    "            configs_dataset[config]['attributes'].extend(attribute_docs)\n",
    "\n",
    "    for config in configs_dataset:\n",
    "        config_str = f\"attribute_context={config[0]}_class_context={config[1]}\"\n",
    "        with open(f\"pmc_data/pmc_{split}_{config_str}.json\", 'w') as f:\n",
    "            json.dump(configs_dataset[config], f, indent=4)\n",
    "    return configs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ce0e436cdf4425b708fb920448d2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_texts = open(TRAIN_MODELS).read().strip().split('\\n')\n",
    "train_configs_dataset = create_dataset(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41205ce058dc4981be8b486edbd69e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_texts = open(TRAIN_MODELS).read().split('\\n')\n",
    "test_text_full_context = open(TEST_MODELS_FULL_CONTEXT).read().strip().split('\\n')\n",
    "test_configs_dataset = create_dataset(test_text_full_context, split='test_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_context = [0, 1]\n",
    "class_context = [0, 1]\n",
    "\n",
    "configs = [c for c in itertools.product(attribute_context, class_context)]\n",
    "train_configs_dataset = {c: json.load(open(f\"pmc_data/pmc_train_attribute_context={c[0]}_class_context={c[1]}.json\")) for c in configs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tf_idf_recommender(train_data, test_data, k=10):\n",
    "\n",
    "    train_x, train_y = [x[0] for x in train_data], [x[1] for x in train_data]\n",
    "    test_x, test_y = [x[0] for x in test_data], [x[1] for x in test_data]\n",
    "\n",
    "    classifier = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('svm', SVC(kernel='linear'))\n",
    "    ])\n",
    "\n",
    "    classifier.fit(train_x, train_y)\n",
    "    probabilities = classifier.predict_proba(test_x)\n",
    "\n",
    "    topk_predictions = [np.argsort(-probs)[:k] for probs in probabilities]\n",
    "\n",
    "    \n",
    "    recall_k = [1 if y_act in y_pred else 0 for y_act, y_pred in zip(test_y, topk_predictions)]\n",
    "    recall = np.sum(recall_k)/len(recall_k)\n",
    "\n",
    "        \n",
    "    print(f\"Recall: {recall}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52488 54158 44261\n",
      "1582 2669 1150\n",
      "Config:  attribute_context=0_class_context=0\n",
      "Classification:  class\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(list_of_lists):\n",
    "    # print(\"Before\", len(list_of_lists))\n",
    "    reverse_map = {str(i): i for i in list_of_lists}\n",
    "    # print(\"After\", len(reverse_map))\n",
    "    return list(reverse_map.values())\n",
    "\n",
    "def get_dataset(configs_dataset, config):\n",
    "    config_data = configs_dataset[config]\n",
    "    class_docs = remove_duplicates(config_data['class'])\n",
    "    association_docs = remove_duplicates(config_data['associations'])\n",
    "    attribute_docs = remove_duplicates(config_data['attributes'])\n",
    "    print(len(class_docs), len(association_docs), len(attribute_docs))\n",
    "    return {\n",
    "        'class': class_docs,\n",
    "        'associations': association_docs,\n",
    "        'attributes': attribute_docs\n",
    "    }\n",
    "\n",
    "for config in configs:\n",
    "    train_data = get_dataset(train_configs_dataset, config)\n",
    "    test_data = get_dataset(test_configs_dataset, config)\n",
    "    config_str = f\"attribute_context={config[0]}_class_context={config[1]}\"\n",
    "    print(\"Config: \", config_str)\n",
    "    for key in train_data:\n",
    "        print(\"Classification: \", key)\n",
    "        tf_idf_recommender(train_data[key], test_data[key])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CM-GNN-VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
