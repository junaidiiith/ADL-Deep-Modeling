{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from parameters import parse_args\n",
    "from graph_utils import get_graph_data\n",
    "from data_generation_utils import get_kfold_lm_data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Getting the data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "data_dir = args.data_dir\n",
    "args.graphs_file = os.path.join(data_dir, args.graphs_file)\n",
    "\n",
    "\n",
    "graph_data = get_graph_data(args.graphs_file)\n",
    "label_map, super_type_map = graph_data['entities_encoder'], graph_data['super_types_encoder']\n",
    "inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "inverse_super_type_map = {v: k for k, v in super_type_map.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99556 11062 6847\n"
     ]
    }
   ],
   "source": [
    "label_map, super_type_map = graph_data['entities_encoder'], graph_data['super_types_encoder']\n",
    "for i, data in enumerate(get_kfold_lm_data(graph_data, seed=args.seed)):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSP = \"<superType>\"\n",
    "ESP = \"</superType>\"\n",
    "SEN = \"<entity>\"\n",
    "EEN = \"</entity>\"\n",
    "\n",
    "SRP = \"<relations>\"\n",
    "ERP = \"</relations>\"\n",
    "\n",
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "SOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "MASK = \"<mask>\"\n",
    "SEP = \"<sep>\"\n",
    "\n",
    "SPECIAL_TOKENS = [PAD, UNK, SOS, EOS, MASK, SEP, SSP, ESP, SEN, EEN, SRP, ERP]\n",
    "\n",
    "clean_text = lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x).strip()\n",
    "\n",
    "promptize_triple = lambda x: f\"{SOS} {SSP} {clean_text(x[2])} {ESP} {SEN} {clean_text(x[0])} {EEN} {SRP} {clean_text(x[1])} {ERP} {EOS}\"\n",
    "\n",
    "def promptize_super_type_generation(x):\n",
    "    return f\"{SOS} {SEN} {clean_text(x[0])} {EEN} {SRP} {clean_text(x[1])} {ERP} {SEP} {SSP} {clean_text(x[2])} {ESP} {EOS}\"\n",
    "\n",
    "def promptize_entity_type_generation(x):\n",
    "    return f\"{SOS} {SSP} {clean_text(x[1])} {ESP} {SRP} {clean_text(x[1])} {ERP} {SEP} {SEN} {clean_text(x[0])} {EEN} {EOS}\"\n",
    "\n",
    "def promptize_super_type_classification(x):\n",
    "    return f\"{SOS} {SEN} {clean_text(x[0])} {EEN} {SRP} {clean_text(x[1])} {ERP} {EOS}\", f\"{clean_text(x[2])}\".split()\n",
    "\n",
    "def promptize_entity_type_classification(x):\n",
    "    return f\"{SOS} {SSP} {clean_text(x[1])} {ESP} {SRP} {clean_text(x[1])} {ERP} {EOS}\", f\"{clean_text(x[0])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(data):\n",
    "    return list({str(i): i for i in data}.values())\n",
    "\n",
    "def print_sample_data(data):\n",
    "    for split_type in data:\n",
    "        print(f\"Split type: {split_type}\")\n",
    "        print(f\"Total number of samples: {len(data[split_type])}\")\n",
    "        print(f\"2 Samples: {data[split_type][:2]}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def get_promptized_data_for_super_type_generation(data):\n",
    "    promptized_data = {\n",
    "        split_type: remove_duplicates([promptize_super_type_generation(i) for i in data[split_type] if len(i[2].strip())])\\\n",
    "              for split_type in data\n",
    "    }\n",
    "    # print_sample_data(promptized_data)\n",
    "    \n",
    "    return promptized_data\n",
    "\n",
    "def get_promptized_data_for_entity_generation(data):\n",
    "    promptized_data = {\n",
    "        split_type: remove_duplicates([promptize_entity_type_generation(i) for i in data[split_type] if len(i[1].strip())])\\\n",
    "              for split_type in data\n",
    "    }\n",
    "    # print_sample_data(promptized_data)\n",
    "    return promptized_data\n",
    "\n",
    "def get_promptized_data_for_super_type_classification(data):\n",
    "    promptized_data = {\n",
    "        split_type: remove_duplicates([promptize_super_type_classification(i) for i in data[split_type] if len(i[2].strip())])\\\n",
    "              for split_type in data\n",
    "    }\n",
    "    print_sample_data(promptized_data)\n",
    "    \n",
    "    return promptized_data\n",
    "\n",
    "def get_promptized_data_for_entity_classification(data):\n",
    "    promptized_data = {\n",
    "        split_type: remove_duplicates([promptize_entity_type_classification(i) for i in data[split_type] if len(i[1].strip())])\\\n",
    "              for split_type in data\n",
    "    }\n",
    "    print_sample_data(promptized_data)\n",
    "    return promptized_data\n",
    "\n",
    "def get_promptized_data_for_generation(data):\n",
    "    data_for_super_type_generation = get_promptized_data_for_super_type_generation(data)\n",
    "    data_for_entity_generation = get_promptized_data_for_entity_generation(data)\n",
    "\n",
    "    promptized_data = {\n",
    "        split_type: data_for_super_type_generation[split_type] + data_for_entity_generation[split_type]\\\n",
    "              for split_type in data\n",
    "    }\n",
    "    print_sample_data(promptized_data)\n",
    "    \n",
    "    return promptized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_classification(data, class_type='super'):\n",
    "    if class_type == 'super':\n",
    "        promptized_data = get_promptized_data_for_super_type_classification(data)\n",
    "    else:\n",
    "        promptized_data = get_promptized_data_for_entity_classification(data)\n",
    "    return promptized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promptize for data generation\n",
      "Split type: train\n",
      "Total number of samples: 119493\n",
      "2 Samples: ['<s> <entity> CFlowPointCut </entity> <relations>  </relations> <sep> <superType> PointCut PointCutPointCut </superType> </s>', '<s> <entity> IfcCartesianPoint </entity> <relations>  </relations> <sep> <superType> IfcLayeredItem IfcGeometricRepresentationItem IfcPointOrVertexPoint IfcPoint IfcGeometricSetSelect </superType> </s>']\n",
      "\n",
      "Split type: test\n",
      "Total number of samples: 13708\n",
      "2 Samples: ['<s> <entity> Contributioncoauthor </entity> <relations>  </relations> <sep> <superType> Thing Regularauthor </superType> </s>', '<s> <entity> Roadmap </entity> <relations>  </relations> <sep> <superType> VariabilityElement DescribableElement Classifier Type Element </superType> </s>']\n",
      "\n",
      "Split type: unseen\n",
      "Total number of samples: 8618\n",
      "2 Samples: ['<s> <entity> ConstantActivity </entity> <relations> value Value </relations> <sep> <superType> Activity </superType> </s>', '<s> <entity> AdaptationTask </entity> <relations> fulfillmentCriteria FulfillmentCriterion </relations> <sep> <superType> AdaptationStatusItem AnnotationBase </superType> </s>']\n",
      "\n",
      "\n",
      "Promptize for super type classification\n",
      "Split type: train\n",
      "Total number of samples: 74311\n",
      "2 Samples: [('<s> <entity> CFlowPointCut </entity> <relations>  </relations> </s>', ['PointCut', 'PointCutPointCut']), ('<s> <entity> IfcCartesianPoint </entity> <relations>  </relations> </s>', ['IfcLayeredItem', 'IfcGeometricRepresentationItem', 'IfcPointOrVertexPoint', 'IfcPoint', 'IfcGeometricSetSelect'])]\n",
      "\n",
      "Split type: test\n",
      "Total number of samples: 8223\n",
      "2 Samples: [('<s> <entity> Contributioncoauthor </entity> <relations>  </relations> </s>', ['Thing', 'Regularauthor']), ('<s> <entity> Roadmap </entity> <relations>  </relations> </s>', ['VariabilityElement', 'DescribableElement', 'Classifier', 'Type', 'Element'])]\n",
      "\n",
      "Split type: unseen\n",
      "Total number of samples: 5061\n",
      "2 Samples: [('<s> <entity> ConstantActivity </entity> <relations> value Value </relations> </s>', ['Activity']), ('<s> <entity> AdaptationTask </entity> <relations> fulfillmentCriteria FulfillmentCriterion </relations> </s>', ['AdaptationStatusItem', 'AnnotationBase'])]\n",
      "\n",
      "\n",
      "Promptize for entity classification\n",
      "Split type: train\n",
      "Total number of samples: 45182\n",
      "2 Samples: [('<s> <superType> condition Condition </superType> <relations> condition Condition </relations> </s>', 'ConditionalLoop'), ('<s> <superType> packages Package </superType> <relations> packages Package </relations> </s>', 'Project')]\n",
      "\n",
      "Split type: test\n",
      "Total number of samples: 5485\n",
      "2 Samples: [('<s> <superType> accounts Account </superType> <relations> accounts Account </relations> </s>', 'Distributor'), ('<s> <superType> unImporteTotal Importe </superType> <relations> unImporteTotal Importe </relations> </s>', 'TotalDeCompra')]\n",
      "\n",
      "Split type: unseen\n",
      "Total number of samples: 3557\n",
      "2 Samples: [('<s> <superType> value Value </superType> <relations> value Value </relations> </s>', 'ConstantActivity'), ('<s> <superType> fulfillmentCriteria FulfillmentCriterion </superType> <relations> fulfillmentCriteria FulfillmentCriterion </relations> </s>', 'AdaptationTask')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Promptize for data generation\")\n",
    "promptized_data = get_promptized_data_for_generation(data)\n",
    "\n",
    "print()\n",
    "print(\"Promptize for super type classification\")\n",
    "data_for_super_type_classification = get_promptized_data_for_super_type_classification(data)\n",
    "\n",
    "print()\n",
    "print(\"Promptize for entity classification\")\n",
    "data_for_entity_classification = get_promptized_data_for_entity_classification(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> <entity> StateMachine </entity> <relations>  </relations> <sep> <superType> StructuredClassifier Classifier Type Namespace PackageableElement </superType> </s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptized_data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Creating tokenizers </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabTokenizer:\n",
    "    def __init__(self, data, lower=True, special_tokens=[]):\n",
    "        self.lower = lower\n",
    "        self.vocab = {}\n",
    "        self.special_tokens = special_tokens\n",
    "        \n",
    "        for i in self.special_tokens:\n",
    "            self.vocab[i] = len(self.vocab)\n",
    "\n",
    "        for split_type in data:\n",
    "            for i in data[split_type]:\n",
    "                word = \" \".join(i) if isinstance(i, tuple) else i\n",
    "                for j in word_tokenize(clean_text(word) if not self.lower else clean_text(word).lower()):\n",
    "                    if j not in self.vocab:\n",
    "                        self.vocab[j] = len(self.vocab)\n",
    "        \n",
    "        self.pad_token_id = self.vocab[PAD]\n",
    "        self.pad_token = PAD\n",
    "\n",
    "        self.unknown_token_id = self.vocab[UNK]\n",
    "        self.unknown_token = UNK\n",
    "        \n",
    "        self.index_to_key = {v: k for k, v in self.vocab.items()}\n",
    "    \n",
    "    def batch_encode(self, x, return_tensors=None, max_length=None):\n",
    "        assert isinstance(x, list), \"Input must be a list\"\n",
    "        batch_encodings = [self.encode(i) for i in tqdm(x, desc='Encoding')]\n",
    "        lengths = [len(i) for i in batch_encodings]\n",
    "        perc_max_length = int(np.percentile(lengths, 99.95))\n",
    "        max_length = 512 if max_length is None else (perc_max_length if max_length == 'percentile' else max_length)\n",
    "        max_length = min(max_length, max([len(i) for i in batch_encodings]))\n",
    "        \n",
    "        batch_input_ids = [i[:min(max_length, len(i))] + [self.pad_token_id] * (max_length - min(max_length, len(i))) for i in batch_encodings]\n",
    "        batch_attention_mask = [[1] * min(max_length, len(i)) + [0] * (max_length - min(max_length, len(i))) for i in batch_encodings]\n",
    "\n",
    "        if return_tensors == 'pt':\n",
    "            return {\n",
    "                'input_ids': torch.LongTensor(batch_input_ids),\n",
    "                'attention_mask': torch.LongTensor(batch_attention_mask)\n",
    "            }\n",
    "        elif return_tensors == 'np':\n",
    "            return {\n",
    "                'input_ids': np.array(batch_input_ids),\n",
    "                'attention_mask': np.array(batch_attention_mask)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': batch_input_ids,\n",
    "                'attention_mask': batch_attention_mask\n",
    "            }\n",
    "\n",
    "\n",
    "    def encode(self, x, return_tensors=None):\n",
    "        input_ids = self(x)\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.LongTensor(input_ids)\n",
    "        elif return_tensors == 'np':\n",
    "            return np.array(input_ids)\n",
    "        return input_ids\n",
    "    \n",
    "\n",
    "    def __call__(self, x):\n",
    "        if isinstance(x, tuple) or isinstance(x, list):\n",
    "            x = \" \".join(x)\n",
    "        \n",
    "        words, x = x.split(), list()\n",
    "        for i in range(0, len(words)):\n",
    "            if words[i] in self.special_tokens:\n",
    "                x.append(words[i])\n",
    "            else:\n",
    "                x.extend(word_tokenize(clean_text(words[i]) if not self.lower else clean_text(words[i]).lower()))\n",
    "\n",
    "        return [self.vocab.get(i, self.vocab['<unk>']) for i in x]\n",
    "    \n",
    "    def decode(self, x):\n",
    "        assert isinstance(x, list), \"Input must be a list\"\n",
    "        return [self.index_to_key[i] for i in x]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    def add_special_tokens(self, special_tokens):\n",
    "        for i in special_tokens:\n",
    "            if i not in self.vocab:\n",
    "                self.vocab[i] = len(self.vocab)\n",
    "        self.index_to_key = {v: k for k, v in self.vocab.items()}\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"VocabTokenizer(vocab_size={len(self.vocab)})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_lm_tokenizer(model_name, special_tokens=SPECIAL_TOKENS):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "    print(\"Vocab size: \", len(tokenizer))\n",
    "    return tokenizer\n",
    "\n",
    "def get_word_tokenizer_tokenizer(data, lower=True, special_tokens=SPECIAL_TOKENS):\n",
    "    tokenizer = VocabTokenizer(data, lower=lower, special_tokens=special_tokens)\n",
    "    print(\"Vocab size: \", len(tokenizer))\n",
    "    return tokenizer\n",
    "\n",
    "# Testing Tokenizer\n",
    "# txt = data['train'][7]\n",
    "# print(txt)\n",
    "# print(vocab_tokenizer(txt))\n",
    "# print(vocab_tokenizer.decode(vocab_tokenizer(txt)))\n",
    "\n",
    "# txt = promtized_data['train'][7]\n",
    "# print(txt)\n",
    "# print(vocab_tokenizer(txt))\n",
    "# print(vocab_tokenizer.decode(vocab_tokenizer(txt)))\n",
    "\n",
    "# txt = promtized_data_for_super_type['train'][7]\n",
    "# print(txt)\n",
    "# print(vocab_tokenizer(txt))\n",
    "# print(vocab_tokenizer.decode(vocab_tokenizer(txt)))\n",
    "\n",
    "\n",
    "# txt = promtized_data_for_entity['train'][7]\n",
    "# print(txt)\n",
    "# print(vocab_tokenizer(txt))\n",
    "# print(vocab_tokenizer.decode(vocab_tokenizer(txt)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  76838\n",
      "Vocab size:  29008\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "vocab_tokenizer = get_word_tokenizer_tokenizer(promptized_data)\n",
    "pretrained_lm_tokenizer = get_pretrained_lm_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating Dataset and DataLoaders </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_super_type_labels(super_types, super_type_map, multi_label=False):\n",
    "    stp_labels = [[super_type_map[j] for j in super_type] for super_type in super_types]\n",
    "    if not multi_label:\n",
    "        stp_labels = np.array([i[0] for i in stp_labels])\n",
    "        stp_labels = torch.from_numpy(stp_labels)\n",
    "    else:\n",
    "        l = list()\n",
    "        for stp_label in stp_labels:\n",
    "            row = torch.zeros(len(super_type_map))\n",
    "            for label in stp_label:\n",
    "                row[label] = 1\n",
    "            l.append(row)\n",
    "            \n",
    "        stp_labels = torch.stack(l)\n",
    "        \n",
    "    return stp_labels\n",
    "\n",
    "def get_encoding_size(data, tokenizer):\n",
    "    tokens = tokenizer(data)\n",
    "    lengths = [len(i) for i in tokens['input_ids']]\n",
    "    return int(np.percentile(lengths, 99.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeUMLDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "        if isinstance(tokenizer, VocabTokenizer):\n",
    "            self.inputs = tokenizer.batch_encode(data, return_tensors='pt', max_length='percentile')\n",
    "        else:\n",
    "            max_token_length = get_encoding_size(data, tokenizer)\n",
    "            self.inputs = tokenizer(data, padding=True, return_tensors='pt', max_length=max_token_length, truncation=True)\n",
    "        self.labels = self.inputs['input_ids'].clone()\n",
    "        self.labels[self.labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        print(self.labels[0].shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "    \n",
    "\n",
    "class SuperTypeClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, super_type_map, multi_label=False):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        super_type_inputs = [i[0] for i in data]\n",
    "        super_type_labels = [i[1] for i in data]\n",
    "        if isinstance(tokenizer, VocabTokenizer):\n",
    "            self.inputs = tokenizer.batch_encode(super_type_inputs, return_tensors='pt', max_length='percentile')\n",
    "        else:\n",
    "            max_token_length = get_encoding_size(super_type_inputs, tokenizer)\n",
    "            self.inputs = tokenizer(super_type_inputs, padding=True, return_tensors='pt', max_length=max_token_length, truncation=True)\n",
    "        \n",
    "        self.labels = get_super_type_labels(super_type_labels, super_type_map, multi_label=multi_label)\n",
    "        self.i2c = {v: k for k, v in super_type_map.items()}\n",
    "        self.multi_label = multi_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "class EntityClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label_map):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        entity_inputs = [i[0] for i in data]\n",
    "        entity_labels = [i[1] for i in data]\n",
    "        if isinstance(tokenizer, VocabTokenizer):\n",
    "            self.inputs = tokenizer.batch_encode(entity_inputs, return_tensors='pt', max_length='percentile')\n",
    "        else:\n",
    "            max_token_length = get_encoding_size(entity_inputs, tokenizer)\n",
    "            self.inputs = tokenizer(entity_inputs, padding=True, return_tensors='pt', max_length=max_token_length, truncation=True)\n",
    "        self.labels = [label_map[i] for i in entity_labels]\n",
    "        self.i2c = {v: k for k, v in label_map.items()}\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generative_uml_dataset(data, tokenizer):\n",
    "    dataset = {\n",
    "        split_type: GenerativeUMLDataset(data[split_type][:100], tokenizer) for split_type in data\n",
    "    }\n",
    "    return dataset\n",
    "\n",
    "def get_super_type_classification_dataset(data, tokenizer, super_type_map, multi_label=False):\n",
    "    dataset = {\n",
    "        split_type: SuperTypeClassificationDataset(\n",
    "            data[split_type][:100], tokenizer, super_type_map, multi_label=multi_label) for split_type in data\n",
    "    }\n",
    "    return dataset\n",
    "\n",
    "def get_entity_classification_dataset(data, tokenizer, label_map):\n",
    "    dataset = {\n",
    "        split_type: EntityClassificationDataset(\n",
    "            data[split_type][:100], tokenizer, label_map) for split_type in data\n",
    "    }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_dataset(data, tokenizer, encoder, class_type='super', multi_label=False):\n",
    "    if class_type == 'super':\n",
    "        return get_super_type_classification_dataset(data, tokenizer, encoder, multi_label=multi_label)\n",
    "    else:\n",
    "        return get_entity_classification_dataset(data, tokenizer, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d09de678eb147729fb674174b379a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094c979285e040a9a81df17c1b1a521c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8156acd19184b0e836ca099665d5c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22])\n"
     ]
    }
   ],
   "source": [
    "vocab_dataset = get_generative_uml_dataset(promptized_data, vocab_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58])\n",
      "torch.Size([98])\n",
      "torch.Size([116])\n"
     ]
    }
   ],
   "source": [
    "lm_dataset = get_generative_uml_dataset(promptized_data, pretrained_lm_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 28998, 29004,  1426,  2107, 19226,  1673, 29005, 29006, 29007,\n",
       "         29001, 29002, 25341,  1181,  1658, 17223, 17792,  3699, 17792,  6902,\n",
       "         13313, 12204, 14667,  2553,  1895,  2036, 20041, 29003, 28999,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([  101, 28998, 29004,  1426,  2107, 19226,  1673, 29005, 29006, 29007,\n",
       "         29001, 29002, 25341,  1181,  1658, 17223, 17792,  3699, 17792,  6902,\n",
       "         13313, 12204, 14667,  2553,  1895,  2036, 20041, 29003, 28999,   102,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_dataset['train']), len(vocab_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def load_dataset(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "100 100\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(lm_dataset['train']), len(vocab_dataset['train']))\n",
    "print(len(lm_dataset['test']), len(vocab_dataset['test']))\n",
    "print(len(lm_dataset['unseen']), len(vocab_dataset['unseen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GenerativeUMLDataset at 0x7fd525a4ce50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StructuredClassifier',\n",
       " 'Classifier',\n",
       " 'Type',\n",
       " 'Namespace',\n",
       " 'PackageableElement']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_super_type_classification['train'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_types_map = {k: i for i, k in enumerate({j for _, data in data_for_super_type_classification.items() for sp in data for j in sp[1]})}\n",
    "entity_map = {k: i for i, k in enumerate({en[1] for _, data in data_for_entity_classification.items() for en in data})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8b409855ad42149b1b912cb3e78093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf0b5b8fd444bffa7f9acf351d2460b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecf2898b66c4df39dcc1828bea7d568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_super_classification_dataset = get_super_type_classification_dataset(\n",
    "    data_for_super_type_classification, vocab_tokenizer, super_types_map)\n",
    "\n",
    "lm_super_classification_dataset = get_super_type_classification_dataset(\n",
    "    data_for_super_type_classification, pretrained_lm_tokenizer, super_types_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6705baccd50a4323b5dea3a9a53f72c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c722cf95e8b49a186cee0e780e29d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b054056d5184719b5016f8676bf3e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_entity_classification_dataset = get_entity_classification_dataset(\n",
    "    data_for_entity_classification, vocab_tokenizer, entity_map)\n",
    "\n",
    "lm_entity_classification_dataset = get_entity_classification_dataset(\n",
    "    data_for_entity_classification, pretrained_lm_tokenizer, entity_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "100 100\n",
      "100 100\n",
      "100 100\n",
      "100 100\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_super_classification_dataset['train']), len(lm_super_classification_dataset['train']))\n",
    "print(len(vocab_super_classification_dataset['test']), len(lm_super_classification_dataset['test']))\n",
    "print(len(vocab_super_classification_dataset['unseen']), len(lm_super_classification_dataset['unseen']))\n",
    "\n",
    "print(len(vocab_entity_classification_dataset['train']), len(lm_entity_classification_dataset['train']))\n",
    "print(len(vocab_entity_classification_dataset['test']), len(lm_entity_classification_dataset['test']))\n",
    "print(len(vocab_entity_classification_dataset['unseen']), len(lm_entity_classification_dataset['unseen']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Saving datasets </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training Language Models </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Creating custom GPT </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd54f484b50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def weights_init(model):\n",
    "    if isinstance(model, nn.Linear):\n",
    "        nn.init.xavier_uniform_(model.weight.data)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias.data)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, head_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(head_size, head_size)))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "\n",
    "        # Compute attention scores (\"affinities\") only where the mask is non-zero\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill((attention_mask.unsqueeze(1) == 0), float('-inf'))  # (B, T, T)\n",
    "        wei = self.softmax(wei)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        head_size = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([Head(embed_dim, head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        out = torch.cat([h(x, attn_mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim=None, num_classes=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        if embed_dim is None:\n",
    "            embed_dim = input_dim\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim if num_classes is None else num_classes),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, n_head):\n",
    "        # embed_dim: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(embed_dim, n_head)\n",
    "        self.ffwd = FeedFoward(embed_dim)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        x = x + self.sa(self.ln1(x), attn_mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class UMLGPT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, n_layer, n_head, load_pretrained_from=None):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "\n",
    "        if load_pretrained_from is not None:\n",
    "            self.load_pretrained(load_pretrained_from)\n",
    "        else:\n",
    "            self.token_embedding_table = nn.Embedding(vocab_size, embed_dim)\n",
    "            self.position_embedding_table = nn.Embedding(block_size, embed_dim)\n",
    "            self.blocks = nn.Sequential(*[Block(embed_dim, n_head) for _ in range(n_layer)])\n",
    "            self.ln_f = nn.LayerNorm(embed_dim) # final layer norm\n",
    "            self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "            self.apply(weights_init)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        embeddings = self.get_embedding(x, attention_mask)\n",
    "        logits = self.lm_head(embeddings)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def get_loss(self, logits, labels, ignore_index=-100):\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_embedding(self, x, attention_mask):\n",
    "        # x: [batch_size, seq_len]\n",
    "        # attention_mask: [batch_size, seq_len]\n",
    "        token_embeddings = self.token_embedding_table(x)\n",
    "        position_ids = torch.arange(x.size(1), dtype=torch.long, device=x.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(x)\n",
    "        position_embeddings = self.position_embedding_table(position_ids)\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "        # # Modify the forward pass to include src_key_padding_mask\n",
    "        for block in self.blocks:\n",
    "            embeddings = block(embeddings, attention_mask)\n",
    "\n",
    "        embeddings = self.ln_f(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def get_model_size(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return super().__repr__() + f'\\nNumber of parameters: {self.get_model_size() / 1000000:.3f}M'\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_pretrained(state_dict_pth):\n",
    "        state_dict = torch.load(state_dict_pth, map_location=device)\n",
    "        vocab_size, embed_dim = [s.shape for _, s in state_dict.items() if 'token_embedding_table' in _][0]\n",
    "        num_heads = max([int(name.split('.sa.heads.')[1].split('.')[0]) for name, s in state_dict.items() if '.sa.heads.' in name]) + 1\n",
    "        block_size = [s.shape[0] for _, s in state_dict.items() if 'position_embedding_table' in _][0]\n",
    "        num_layers = max([int(name.split('blocks.')[1].split('.')[0]) for name, s in state_dict.items() if 'blocks.' in name]) + 1\n",
    "        model = UMLGPT(vocab_size, embed_dim, block_size, num_layers, num_heads)\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76838, 29008)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_tokenizer), len(pretrained_lm_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = 1e-3\n",
    "args.batch_size = 32\n",
    "args.epochs = 1\n",
    "args.log_dir = 'logs'\n",
    "args.models_dir = 'models'\n",
    "args.from_pretrained = None\n",
    "args.embed_dim = 128\n",
    "args.block_size = 512\n",
    "args.num_layers = 1\n",
    "args.num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uml_gpt(input_dim, args):\n",
    "    embed_dim = args.embed_dim\n",
    "    n_layer = args.num_layers\n",
    "    n_head = args.num_heads\n",
    "    block_size = args.block_size\n",
    "\n",
    "    uml_gpt = UMLGPT(input_dim, embed_dim, block_size, n_layer, n_head)\n",
    "    if args.from_pretrained is not None:\n",
    "        uml_gpt.load_state_dict(torch.load(os.path.join(args.models_dir, args.from_pretrained)))\n",
    "        print(f'Loaded pretrained model from {args.from_pretrained}')\n",
    "    \n",
    "    uml_gpt.to(device)\n",
    "    return uml_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.from_pretrained = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm_uml_gpt = get_uml_gpt(len(pretrained_lm_tokenizer), args)\n",
    "vocab_uml_gpt = get_uml_gpt(len(vocab_tokenizer), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(dataset, batch_size=32):\n",
    "    dataloaders = {\n",
    "        split_type: DataLoader(\n",
    "            dataset[split_type], batch_size=batch_size, shuffle=split_type == 'train') for split_type in dataset\n",
    "    }\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class UMLGPTTrainer:\n",
    "    def __init__(self, model, dataloaders, args, compute_metrics_fn=None):\n",
    "        self.model = model\n",
    "        self.lr = args.lr\n",
    "        self.batch_size = args.batch_size\n",
    "        self.dataloaders = dataloaders\n",
    "        self.args = args\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=args.num_epochs)\n",
    "        self.writer = SummaryWriter(log_dir=args.log_dir)\n",
    "        self.models_dir = args.models_dir\n",
    "        self.model_str = self.model_str = f'{self.model._get_name()}_Vocab{args.trainer}'\n",
    "        self.compute_metrics_fn = compute_metrics_fn\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        self.model.train()\n",
    "                    \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            best_test_loss = float('inf')\n",
    "            epoch_metrics = {'loss': 0}\n",
    "            for i, batch in tqdm(enumerate(self.dataloaders['train']), desc=f'Epoch {epoch}', total=len(self.dataloaders['train'])):\n",
    "                loss, logits, labels = self.step(batch)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                epoch_metrics['loss'] += epoch_loss\n",
    "\n",
    "                if self.compute_metrics_fn is not None:\n",
    "                    metrics = self.compute_metrics_fn(logits, labels)\n",
    "                    for metric in metrics:\n",
    "                        if metric not in epoch_metrics:\n",
    "                            epoch_metrics[metric] = 0\n",
    "                        epoch_metrics[metric] += metrics[metric]\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(f'Epoch {epoch} Batch {i} Avg Loss: {epoch_loss / (i + 1)}')\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "            self.write_metrics(epoch_metrics, epoch, 'train')\n",
    "\n",
    "            test_loss = self.evaluate(epoch, 'test')\n",
    "            self.evaluate(epoch, 'unseen')\n",
    "\n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "                self.save_model(f'{self.model_str}_best_model.pt')\n",
    "                print(f'Best model saved at epoch {epoch}')\n",
    "    \n",
    "                \n",
    "    def evaluate(self, epoch, split_type='test'):\n",
    "        self.model.eval()\n",
    "        eval_metrics = {'loss': 0}\n",
    "        for batch in tqdm(self.dataloaders[split_type], desc=f'Evaluation'):\n",
    "            loss, logits, labels = self.step(batch)\n",
    "\n",
    "            if self.compute_metrics_fn is not None:\n",
    "                metrics = self.compute_metrics_fn(logits, labels)\n",
    "                for metric in metrics:\n",
    "                    if metric not in eval_metrics:\n",
    "                        eval_metrics[metric] = 0\n",
    "                    eval_metrics[metric] += metrics[metric]\n",
    "\n",
    "            eval_metrics['loss'] += loss.item()\n",
    "\n",
    "        for metric in eval_metrics:\n",
    "            if metric != 'loss':\n",
    "                eval_metrics[metric] /= len(self.dataloaders[split_type])\n",
    "\n",
    "        self.write_metrics(eval_metrics, epoch, split_type)\n",
    "        return eval_metrics['loss']\n",
    "    \n",
    "\n",
    "    def step(self, batch):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        logits = self.model(input_ids, attention_mask)\n",
    "        loss = self.model.get_loss(logits, labels)\n",
    "        return loss, logits, labels\n",
    "\n",
    "\n",
    "    def save_model(self, file_name):\n",
    "        if not os.path.exists(self.models_dir):\n",
    "            os.makedirs(self.models_dir)\n",
    "        file_name = os.path.join(self.models_dir, file_name)\n",
    "        torch.save(self.model.state_dict(), file_name)\n",
    "        print(f'Saved model at {file_name}')\n",
    "    \n",
    "    def load_model(self, file_name):\n",
    "        file_name = os.path.join(self.models_dir, file_name)\n",
    "        self.model.load_state_dict(torch.load(file_name))\n",
    "        print(f'Loaded model from {file_name}')\n",
    "    \n",
    "    \n",
    "    def write_metrics(self, metrics, epoch, split_type):\n",
    "        print(f'Epoch {epoch} {split_type} metrics: ', end='')\n",
    "        for metric in metrics:\n",
    "            self.writer.add_scalar(f'Metrics/{split_type}{metric}', metrics[metric], epoch)\n",
    "            print(f'{metric}: {metrics[metric]:.3f}', end=' ')\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.trainer = 'PT'\n",
    "lm_trainer = UMLGPTTrainer(plm_uml_gpt, get_dataloaders(lm_dataset), args)\n",
    "args.trainer = 'CT'\n",
    "vocab_trainer = UMLGPTTrainer(vocab_uml_gpt, get_dataloaders(vocab_dataset), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UMLGPT(\n",
       "  (token_embedding_table): Embedding(29008, 128)\n",
       "  (position_embedding_table): Embedding(512, 128)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=128, out_features=29008, bias=True)\n",
       ")\n",
       "Number of parameters: 7.719M"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plm_uml_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e91d469a27e4aba9992000885321947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0 Avg Loss: 10.279776573181152\n",
      "Epoch 0 train metrics: loss: 101.950 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030a8f22bedd4c8cac02fb7938f150d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 test metrics: loss: 39.544 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02180a4cb9264fd89fcfd873f075e32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 unseen metrics: loss: 39.641 \n",
      "Saved model at models/UMLGPT_VocabPT_best_model.pt\n",
      "Best model saved at epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3408682e3a5448e38fff1a4668171acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0 Avg Loss: 11.228178977966309\n",
      "Epoch 0 train metrics: loss: 111.063 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ac43c87275429c8332851160e4e876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 test metrics: loss: 42.913 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c845844a8e564473aa87abd9012745e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 unseen metrics: loss: 42.931 \n",
      "Saved model at models/UMLGPT_VocabCT_best_model.pt\n",
      "Best model saved at epoch 0\n"
     ]
    }
   ],
   "source": [
    "lm_trainer.train(args.epochs)\n",
    "vocab_trainer.train(args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "    def __init__(self, tokenized):\n",
    "        self.tokenized = tokenized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = {key: val[index] for key, val in self.tokenized.items()}\n",
    "        return item\n",
    "    \n",
    "def get_gpt2_tokenized_data(data, tokenizer):\n",
    "    tokenized_data = {\n",
    "        split_type: tokenizer(\n",
    "            data[split_type], \n",
    "            padding=True, \n",
    "            return_tensors='pt', \n",
    "            max_length=get_encoding_size(data[split_type], tokenizer), \n",
    "            truncation=True\n",
    "        ) for split_type in data\n",
    "    }\n",
    "    return tokenized_data\n",
    "\n",
    "def get_gpt2_dataset(data, tokenizer):\n",
    "    tokenized_data = get_gpt2_tokenized_data(data, tokenizer)\n",
    "    dataset = {\n",
    "        split_type: GPT2Dataset(tokenized_data[split_type]) for split_type in data\n",
    "    }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from transformers.integrations import NeptuneCallback\n",
    "\n",
    "def suppress_neptune(trainer):\n",
    "    for cb in trainer.callback_handler.callbacks:\n",
    "        if isinstance(cb, NeptuneCallback):\n",
    "            trainer.callback_handler.remove_callback(cb)\n",
    "\n",
    "\n",
    "def train_hugging_face_gpt(data, args):\n",
    "    model_name = args.gpt_model\n",
    "    tokenizer = get_pretrained_lm_tokenizer(model_name, special_tokens=args.special_tokens)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    print('Creating dataset...')\n",
    "    dataset = get_gpt2_dataset(data, tokenizer)\n",
    "    print('Done!')\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # Set to True if you want to perform masked language modeling\n",
    "    )\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.log_dir,          # output directory\n",
    "        num_train_epochs=args.num_epochs,              # total number of training epochs\n",
    "        per_device_train_batch_size=args.batch_size,   # batch size per device during training\n",
    "        per_device_eval_batch_size=args.batch_size,    # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir=args.log_dir,            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=1,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=100,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        fp16=True,\n",
    "        greater_is_better=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=dataset['train'],         # training dataset\n",
    "        eval_dataset=dataset['test'],          # evaluation dataset\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    suppress_neptune(trainer)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print('Evaluating on test set...')\n",
    "    trainer.evaluate(dataset['test'])\n",
    "\n",
    "    print('Evaluating on unseen set...')\n",
    "    trainer.evaluate(dataset['unseen'])\n",
    "\n",
    "    trainer.save_model(os.path.join(args.log_dir, f'uml_{model_name}'))\n",
    "\n",
    "\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating Sequence Classification Models </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UMLGPTClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, model, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        _, embed_dim = self.model.lm_head.weight.data.shape\n",
    "        self.classifier = FeedFoward(input_dim=embed_dim, num_classes=num_classes)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x, attention_mask, pool=None):\n",
    "        # x: [batch_size, seq_len]\n",
    "        # attention_mask: [batch_size, seq_len]\n",
    "        lm_logits = self.model.get_embedding(x, attention_mask)\n",
    "        if pool:\n",
    "            \"\"\"Pool the logits across the sequence dimension\"\"\"\n",
    "            lm_logits = torch.mean(lm_logits, dim=1)\n",
    "        else:\n",
    "            \"\"\"Use the logits at the last position\"\"\"\n",
    "            lm_logits = lm_logits[:, -1, :]\n",
    "        \n",
    "        logits = self.classifier(lm_logits)\n",
    "        return logits\n",
    "    \n",
    "    def get_loss(self, logits, labels):\n",
    "        logits = logits.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if len(labels.shape) == 1:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        else:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.float(), labels.float())\n",
    "        return loss\n",
    "\n",
    "    def get_model_size(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return super().__repr__() + f'\\nNumber of parameters: {self.get_model_size()/1000000:.3f}M'\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_pretrained(state_dict, num_classes):\n",
    "        model = UMLGPTClassifier(UMLGPT.from_pretrained(state_dict), num_classes)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "uml_gpt_vocab_entity_classifier = UMLGPTClassifier(vocab_uml_gpt, len(entity_map))\n",
    "uml_gpt_vocab_super_type_classifier = UMLGPTClassifier(vocab_uml_gpt, len(super_types_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "uml_gpt_plm_entity_classifier = UMLGPTClassifier(plm_uml_gpt, len(entity_map))\n",
    "uml_gpt_plm_super_type_classifier = UMLGPTClassifier(plm_uml_gpt, len(super_types_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UMLGPTClassifier'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uml_gpt_plm_entity_classifier._get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 2,  8, 14,  9, 10, 11,  3,  0,  0,  0,  0,  0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(10806)}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_super_classification_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_recommendation_metrics, get_recommendation_metrics_multi_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_uml_gpt_classification(data, encoder, tokenizer, compute_metrics_fn, args):\n",
    "    data = get_data_for_classification(data, class_type=args.class_type)\n",
    "    dataset = get_classification_dataset(data, tokenizer, encoder, class_type=args.class_type, multi_label=args.multi_label)\n",
    "    model = get_uml_gpt(len(tokenizer), args)\n",
    "    uml_gpt_classifier = UMLGPTClassifier(model, len(encoder))\n",
    "    uml_gpt_trainer = UMLGPTTrainer(uml_gpt_classifier, get_dataloaders(dataset), args, compute_metrics_fn=compute_metrics_fn)\n",
    "    uml_gpt_trainer.train(args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.multi_label = False\n",
    "args.class_type = 'entity'\n",
    "\n",
    "args.from_pretrained = 'UMLGPT_VocabCT_best_model.pt'\n",
    "train_uml_gpt_classification(data, entity_map, vocab_tokenizer, get_recommendation_metrics, args)\n",
    "\n",
    "print('-'*100)\n",
    "\n",
    "args.from_pretrained = 'UMLGPT_VocabPT_best_model.pt'\n",
    "train_uml_gpt_classification(data, entity_map, pretrained_lm_tokenizer, get_recommendation_metrics, args)\n",
    "\n",
    "print('-'*100)\n",
    "print('-'*100)\n",
    "print('-'*100)\n",
    "\n",
    "args.class_type = 'super'\n",
    "\n",
    "args.from_pretrained = 'UMLGPT_VocabCT_best_model.pt'\n",
    "train_uml_gpt_classification(data, super_types_map, vocab_tokenizer, get_recommendation_metrics, args)\n",
    "\n",
    "print('-'*100)\n",
    "\n",
    "args.from_pretrained = 'UMLGPT_VocabPT_best_model.pt'\n",
    "train_uml_gpt_classification(data, super_types_map, pretrained_lm_tokenizer, get_recommendation_metrics, args)\n",
    "\n",
    "print('-'*100)\n",
    "print('-'*100)\n",
    "print('-'*100)\n",
    "\n",
    "\n",
    "args.multi_label = True\n",
    "args.from_pretrained = 'UMLGPT_VocabCT_best_model.pt'\n",
    "train_uml_gpt_classification(data, super_types_map, vocab_tokenizer, get_recommendation_metrics_multi_label, args)\n",
    "\n",
    "print('-'*100)\n",
    "\n",
    "args.from_pretrained = 'UMLGPT_VocabPT_best_model.pt'\n",
    "train_uml_gpt_classification(data, super_types_map, pretrained_lm_tokenizer, get_recommendation_metrics_multi_label, args)\n",
    "\n",
    "print('-'*100)\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import utils\n",
    "\n",
    "def train_hf_for_classification(model_name, tokenizer, dataset, args):\n",
    "    batch_size = args.lm_batch_size\n",
    "    train, test, unseen = dataset['train'], dataset['test'], dataset['unseen']\n",
    "    # Show the training loss with every epoch\n",
    "    logging_steps = len(train) // batch_size\n",
    "    print(f\"Using model...{model_name}\")\n",
    "    model = utils.get_classification_model(model_name, len(dataset.num_labels), tokenizer)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(\"Finetuning model...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.out_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        fp16=True,\n",
    "        logging_steps=logging_steps,\n",
    "        num_train_epochs=args.num_epochs,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=test,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=utils.compute_metrics,\n",
    "    )\n",
    "    for cb in trainer.callback_handler.callbacks:\n",
    "        if isinstance(cb, transformers.integrations.NeptuneCallback):\n",
    "            trainer.callback_handler.remove_callback(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('models/UMLGPT_VocabPT_best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['token_embedding_table.weight', 'position_embedding_table.weight', 'blocks.0.sa.heads.0.tril', 'blocks.0.sa.heads.0.key.weight', 'blocks.0.sa.heads.0.query.weight', 'blocks.0.sa.heads.0.value.weight', 'blocks.0.sa.heads.1.tril', 'blocks.0.sa.heads.1.key.weight', 'blocks.0.sa.heads.1.query.weight', 'blocks.0.sa.heads.1.value.weight', 'blocks.0.sa.heads.2.tril', 'blocks.0.sa.heads.2.key.weight', 'blocks.0.sa.heads.2.query.weight', 'blocks.0.sa.heads.2.value.weight', 'blocks.0.sa.heads.3.tril', 'blocks.0.sa.heads.3.key.weight', 'blocks.0.sa.heads.3.query.weight', 'blocks.0.sa.heads.3.value.weight', 'blocks.0.sa.heads.4.tril', 'blocks.0.sa.heads.4.key.weight', 'blocks.0.sa.heads.4.query.weight', 'blocks.0.sa.heads.4.value.weight', 'blocks.0.sa.heads.5.tril', 'blocks.0.sa.heads.5.key.weight', 'blocks.0.sa.heads.5.query.weight', 'blocks.0.sa.heads.5.value.weight', 'blocks.0.sa.heads.6.tril', 'blocks.0.sa.heads.6.key.weight', 'blocks.0.sa.heads.6.query.weight', 'blocks.0.sa.heads.6.value.weight', 'blocks.0.sa.heads.7.tril', 'blocks.0.sa.heads.7.key.weight', 'blocks.0.sa.heads.7.query.weight', 'blocks.0.sa.heads.7.value.weight', 'blocks.0.sa.proj.weight', 'blocks.0.sa.proj.bias', 'blocks.0.ffwd.net.0.weight', 'blocks.0.ffwd.net.0.bias', 'blocks.0.ffwd.net.2.weight', 'blocks.0.ffwd.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'ln_f.weight', 'ln_f.bias', 'lm_head.weight', 'lm_head.bias'])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given X, y pairs\n",
    "Create TF-IDF Vectorizor for the data and then use SVM to classify the data\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def get_tfidf_vectorizer(data):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(data)\n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "def get_Xy(data, split):\n",
    "    X = [j[0] for j in data[split]]\n",
    "    y = [j[1] for j in data[split]]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def get_svm_classifier(X, y):\n",
    "    clf = SVC()\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def get_svm_metrics(clf, X, y):\n",
    "    X, y = get_Xy(data)\n",
    "    y_pred = clf.predict(X)\n",
    "    print(classification_report(y, y_pred))\n",
    "    print(f'F1 Score: {f1_score(y, y_pred, average=\"macro\")}')\n",
    "    print(f'Precision Score: {precision_score(y, y_pred, average=\"macro\")}')\n",
    "    print(f'Recall Score: {recall_score(y, y_pred, average=\"macro\")}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s> <entity> CFlowPointCut </entity> <relations>  </relations> </s>',\n",
       " 'PointCut')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "\n",
    "Xy = [(j[0], k) for j in data_for_super_type_classification['train'] for k in j[1]] \\\n",
    "        + [(j[0], k) for j in data_for_super_type_classification['test'] for k in j[1]] \\\n",
    "            + [(j[0], k) for j in data_for_super_type_classification['unseen'] for k in j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = get_tfidf_vectorizer([i[0] for i in Xy])\n",
    "X = tf_idf_vectorizer.transform([i[0] for i in Xy])\n",
    "\n",
    "y = LabelEncoder().fit_transform([i[1] for i in Xy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2c = {k: v for v, k in zip(y, [i[1] for i in Xy])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train = [(j[0], k) for j in data_for_super_type_classification['train'] for k in j[1]]\n",
    "Xy_test = [(j[0], k) for j in data_for_super_type_classification['test'] for k in j[1]]\n",
    "Xy_unseen = [(j[0], k) for j in data_for_super_type_classification['unseen'] for k in j[1]]\n",
    "\n",
    "X_train = tf_idf_vectorizer.transform([i[0] for i in Xy_train])\n",
    "X_test = tf_idf_vectorizer.transform([i[0] for i in Xy_test])\n",
    "X_unseen = tf_idf_vectorizer.transform([i[0] for i in Xy_unseen])\n",
    "\n",
    "y_train = np.array([i2c[i[1]] for i in Xy_train])\n",
    "y_test = np.array([i2c[i[1]] for i in Xy_test])\n",
    "y_unseen = np.array([i2c[i[1]] for i in Xy_unseen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CM-GNN-VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
